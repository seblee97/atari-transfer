{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning Analysis for Atari Games\n",
    "\n",
    "This notebook analyzes transfer learning results including:\n",
    "1. Learning curves for each experiment\n",
    "2. Transfer benefit analysis (comparing transfer vs from-scratch performance)\n",
    "3. Visualizations split by algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Define the results directory and experiment parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nRESULTS_DIR = \"results\"  # Transfer learning results\nBASELINE_DIR = \"results_baseline\"  # Baseline (from-scratch) results\nOUTPUT_DIR = \"analysis_plots\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Games and algorithms from your config\nGAMES = [\"Pong\", \"Breakout\", \"SpaceInvaders\", \"Tennis\"]\nALGORITHMS = [\"dqn\", \"ppo\", \"qrdqn\"]\n\n# Metrics to extract from TensorBoard\nMETRICS = [\n    \"rollout/ep_rew_mean\",  # Episode reward\n    \"rollout/ep_len_mean\",  # Episode length\n    \"train/loss\",            # Training loss\n]\n\nprint(f\"Transfer results directory: {RESULTS_DIR}\")\nprint(f\"Baseline results directory: {BASELINE_DIR}\")\nprint(f\"Output directory: {OUTPUT_DIR}\")\nprint(f\"Plots will be saved to: {os.path.abspath(OUTPUT_DIR)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_tensorboard_data(log_dir, metric=\"rollout/ep_rew_mean\"):\n    \"\"\"\n    Load data from TensorBoard event files.\n    \n    Args:\n        log_dir: Path to TensorBoard log directory\n        metric: Metric name to extract\n    \n    Returns:\n        DataFrame with columns: step, value\n    \"\"\"\n    try:\n        ea = event_accumulator.EventAccumulator(log_dir)\n        ea.Reload()\n        \n        if metric not in ea.Tags()['scalars']:\n            print(f\"Warning: Metric '{metric}' not found in {log_dir}\")\n            return pd.DataFrame(columns=['step', 'value'])\n        \n        events = ea.Scalars(metric)\n        data = pd.DataFrame([\n            {'step': e.step, 'value': e.value}\n            for e in events\n        ])\n        return data\n    except Exception as e:\n        print(f\"Error loading {log_dir}: {e}\")\n        return pd.DataFrame(columns=['step', 'value'])\n\n\ndef parse_experiment_name(exp_name):\n    \"\"\"\n    Parse experiment name to extract metadata.\n    \n    Expected format: {algorithm}_{source}_to_{target}_{timestamp}\n    or: {algorithm}_{source}_to_{target}_pretrained_{timestamp}\n    \n    Returns:\n        dict with algorithm, source, target, pretrained, timestamp\n    \"\"\"\n    parts = exp_name.split('_')\n    \n    if len(parts) < 4:\n        return None\n    \n    algorithm = parts[0]\n    \n    # Find 'to' index\n    try:\n        to_idx = parts.index('to')\n    except ValueError:\n        return None\n    \n    source = parts[to_idx - 1]\n    \n    # Check if pretrained\n    if 'pretrained' in parts:\n        pretrained_idx = parts.index('pretrained')\n        target = parts[to_idx + 1]\n        pretrained = True\n        timestamp = '_'.join(parts[pretrained_idx + 1:])\n    else:\n        target = parts[to_idx + 1]\n        pretrained = False\n        timestamp = '_'.join(parts[to_idx + 2:])\n    \n    return {\n        'algorithm': algorithm,\n        'source': source,\n        'target': target,\n        'pretrained': pretrained,\n        'timestamp': timestamp\n    }\n\n\ndef find_experiments(results_dir, pretrained_only=True):\n    \"\"\"\n    Find all experiments in the results directory.\n    \n    Args:\n        results_dir: Path to results directory\n        pretrained_only: If True, only include pretrained experiments\n    \n    Returns:\n        List of dicts with experiment metadata and paths\n        - If multiple experiments exist for the same (algorithm, source, target) \n          combination, only the latest one (by timestamp) is kept\n    \"\"\"\n    experiments = []\n    \n    if not os.path.exists(results_dir):\n        print(f\"Results directory '{results_dir}' not found!\")\n        return experiments\n    \n    for exp_name in os.listdir(results_dir):\n        exp_path = os.path.join(results_dir, exp_name)\n        \n        if not os.path.isdir(exp_path):\n            continue\n        \n        # Skip non-experiment directories\n        if exp_name in ['slurm_scripts', 'slurm_scripts_pretrained', 'slurm_logs']:\n            continue\n        \n        metadata = parse_experiment_name(exp_name)\n        if metadata is None:\n            continue\n        \n        # Filter for pretrained only if requested\n        if pretrained_only and not metadata['pretrained']:\n            continue\n        \n        # Find log directories\n        source_logs = os.path.join(exp_path, 'source_logs')\n        target_logs = os.path.join(exp_path, 'target_logs')\n        \n        metadata['name'] = exp_name\n        metadata['path'] = exp_path\n        metadata['source_logs'] = source_logs if os.path.exists(source_logs) else None\n        metadata['target_logs'] = target_logs if os.path.exists(target_logs) else None\n        \n        experiments.append(metadata)\n    \n    # Keep only the latest experiment for each (algorithm, source, target) combination\n    # Group by (algorithm, source, target)\n    grouped = {}\n    for exp in experiments:\n        key = (exp['algorithm'], exp['source'], exp['target'])\n        if key not in grouped:\n            grouped[key] = []\n        grouped[key].append(exp)\n    \n    # For each group, keep only the one with the latest timestamp\n    latest_experiments = []\n    for key, exps in grouped.items():\n        # Sort by timestamp (lexicographic sort works for format YYYYMMDD_HHMMSS)\n        latest_exp = max(exps, key=lambda e: e['timestamp'])\n        latest_experiments.append(latest_exp)\n    \n    print(f\"\\nFiltering: pretrained_only={pretrained_only}\")\n    print(f\"Found {len(experiments)} total experiments\")\n    print(f\"Kept {len(latest_experiments)} latest unique experiments\")\n    \n    # Show which experiments were kept/discarded\n    if len(experiments) > len(latest_experiments):\n        print(\"\\nDuplicate experiments found (keeping only latest):\")\n        kept_names = set(exp['name'] for exp in latest_experiments)\n        for exp in experiments:\n            if exp['name'] not in kept_names:\n                key = (exp['algorithm'], exp['source'], exp['target'])\n                print(f\"  Discarded: {exp['name']}\")\n    \n    return latest_experiments"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Find all transfer experiments\nexperiments = find_experiments(RESULTS_DIR)\n\nprint(f\"Found {len(experiments)} transfer experiments\")\nprint(\"\\nTransfer experiment summary:\")\nexp_df = pd.DataFrame(experiments)\nif len(exp_df) > 0:\n    print(exp_df[['algorithm', 'source', 'target', 'pretrained']].to_string())\nelse:\n    print(\"No experiments found!\")\n\n# Load baseline experiments\nprint(\"\\n\" + \"=\"*80)\nprint(\"LOADING BASELINE EXPERIMENTS\")\nprint(\"=\"*80)\n\nbaseline_experiments = {}\n\nif os.path.exists(BASELINE_DIR):\n    for exp_name in os.listdir(BASELINE_DIR):\n        exp_path = os.path.join(BASELINE_DIR, exp_name)\n        \n        if not os.path.isdir(exp_path):\n            continue\n        \n        # Parse baseline experiment name: {algorithm}_{game}_baseline_{job_id}\n        parts = exp_name.split('_')\n        if len(parts) < 3 or 'baseline' not in parts:\n            continue\n        \n        algorithm = parts[0]\n        game = parts[1]\n        \n        # Find log directory\n        logs_dir = os.path.join(exp_path, 'logs')\n        if not os.path.exists(logs_dir):\n            continue\n        \n        # Store baseline experiment (keep latest if multiple)\n        key = (algorithm, game)\n        if key not in baseline_experiments or exp_name > baseline_experiments[key]['name']:\n            baseline_experiments[key] = {\n                'algorithm': algorithm,\n                'game': game,\n                'name': exp_name,\n                'path': exp_path,\n                'logs': logs_dir\n            }\n\nprint(f\"\\nFound {len(baseline_experiments)} baseline experiments\")\nif baseline_experiments:\n    print(\"\\nBaseline summary:\")\n    for (algo, game), exp in sorted(baseline_experiments.items()):\n        print(f\"  {algo.upper()}: {game}\")\nelse:\n    print(\"No baseline experiments found. Make sure BASELINE_DIR is correct.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Learning Curves\n",
    "\n",
    "Plot learning curves for each experiment, organized by algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_learning_curves_by_algorithm(experiments, metric=\"rollout/ep_rew_mean\"):\n    \"\"\"\n    Plot learning curves grouped by algorithm with consistent colors and average curves.\n    \"\"\"\n    algorithms = sorted(set(exp['algorithm'] for exp in experiments))\n    \n    # Create consistent color mapping for game combinations\n    # Get all unique games\n    all_games = set()\n    all_pairs = set()\n    for exp in experiments:\n        all_games.add(exp['source'])\n        all_games.add(exp['target'])\n        all_pairs.add((exp['source'], exp['target']))\n    \n    # Create color palettes\n    game_colors = {}\n    pair_colors = {}\n    \n    # Use a consistent colormap\n    source_cmap = plt.cm.get_cmap('tab10')\n    pair_cmap = plt.cm.get_cmap('tab20')\n    \n    for i, game in enumerate(sorted(all_games)):\n        game_colors[game] = source_cmap(i % 10)\n    \n    for i, pair in enumerate(sorted(all_pairs)):\n        pair_colors[pair] = pair_cmap(i % 20)\n    \n    for algo in algorithms:\n        algo_exps = [exp for exp in experiments if exp['algorithm'] == algo]\n        \n        if not algo_exps:\n            continue\n        \n        # Create subplots for source and target\n        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n        fig.suptitle(f'{algo.upper()} Learning Curves - {metric}', fontsize=14, fontweight='bold')\n        \n        # Plot source training\n        ax_source = axes[0]\n        plotted_sources = set()\n        source_data_list = []\n        \n        for exp in algo_exps:\n            if exp['source_logs'] is None or not os.path.exists(exp['source_logs']):\n                continue\n            \n            # Avoid duplicate source game plots\n            if exp['source'] in plotted_sources:\n                continue\n            \n            data = load_tensorboard_data(exp['source_logs'], metric)\n            if len(data) > 0:\n                label = f\"{exp['source']}\"\n                color = game_colors[exp['source']]\n                ax_source.plot(data['step'], data['value'], label=label, \n                             color=color, alpha=0.6, linewidth=1.5)\n                plotted_sources.add(exp['source'])\n                source_data_list.append(data)\n        \n        # Compute and plot average for source\n        if len(source_data_list) > 1:\n            # Interpolate all curves to common timesteps\n            all_steps = sorted(set(step for data in source_data_list for step in data['step']))\n            interp_values = []\n            \n            for data in source_data_list:\n                interp_val = np.interp(all_steps, data['step'], data['value'])\n                interp_values.append(interp_val)\n            \n            avg_values = np.mean(interp_values, axis=0)\n            ax_source.plot(all_steps, avg_values, label='Average', \n                         color='black', linewidth=3, linestyle='--', alpha=0.9, zorder=100)\n        \n        ax_source.set_xlabel('Timesteps', fontsize=11)\n        ax_source.set_ylabel('Reward', fontsize=11)\n        ax_source.set_title('Source Game Training (from scratch)', fontsize=12)\n        ax_source.legend()\n        ax_source.grid(True, alpha=0.3)\n        \n        # Plot target training\n        ax_target = axes[1]\n        target_data_list = []\n        \n        for exp in algo_exps:\n            if exp['target_logs'] is None or not os.path.exists(exp['target_logs']):\n                continue\n            \n            data = load_tensorboard_data(exp['target_logs'], metric)\n            if len(data) > 0:\n                pretrained_tag = \" (pretrained)\" if exp['pretrained'] else \"\"\n                label = f\"{exp['source']} \u2192 {exp['target']}{pretrained_tag}\"\n                color = pair_colors[(exp['source'], exp['target'])]\n                ax_target.plot(data['step'], data['value'], label=label, \n                             color=color, alpha=0.6, linewidth=1.5)\n                target_data_list.append(data)\n        \n        # Compute and plot average for target\n        if len(target_data_list) > 1:\n            # Interpolate all curves to common timesteps\n            all_steps = sorted(set(step for data in target_data_list for step in data['step']))\n            interp_values = []\n            \n            for data in target_data_list:\n                interp_val = np.interp(all_steps, data['step'], data['value'])\n                interp_values.append(interp_val)\n            \n            avg_values = np.mean(interp_values, axis=0)\n            ax_target.plot(all_steps, avg_values, label='Average', \n                         color='black', linewidth=3, linestyle='--', alpha=0.9, zorder=100)\n        \n        ax_target.set_xlabel('Timesteps', fontsize=11)\n        ax_target.set_ylabel('Reward', fontsize=11)\n        ax_target.set_title('Target Game Training (with transfer)', fontsize=12)\n        ax_target.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n        ax_target.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.savefig(os.path.join(OUTPUT_DIR, f'learning_curves_{algo}.png'), dpi=300, bbox_inches='tight')\n        plt.show()\n\n# Plot learning curves\nplot_learning_curves_by_algorithm(experiments)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Individual Transfer Comparisons\n",
    "\n",
    "For each transfer pair, compare source (baseline) vs target (transfer) performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def smooth_curve(data, window_size=10):\n    \"\"\"\n    Smooth a curve using a moving average.\n    \n    Args:\n        data: Array of values to smooth\n        window_size: Size of the smoothing window\n    \n    Returns:\n        Smoothed array\n    \"\"\"\n    if len(data) < window_size:\n        return data\n    \n    # Use pandas for efficient rolling mean\n    smoothed = pd.Series(data).rolling(window=window_size, min_periods=1, center=True).mean()\n    return smoothed.values\n\n\ndef plot_transfer_vs_baseline(experiments, baseline_experiments, metric=\"rollout/ep_rew_mean\", \n                              smooth_window=10):\n    \"\"\"\n    Plot transfer learning vs baseline with improvement metric.\n    \n    For each transfer experiment, plot:\n    1. Transfer learning curve (target training with pretrained source)\n    2. Baseline learning curve (training from scratch)\n    3. Difference/improvement over time (with average across all transfer experiments)\n    \n    Args:\n        experiments: List of transfer experiments\n        baseline_experiments: Dict of baseline experiments\n        metric: Metric to plot\n        smooth_window: Window size for smoothing the average curve\n    \"\"\"\n    # Group by algorithm and target game\n    transfer_by_target = {}\n    for exp in experiments:\n        key = (exp['algorithm'], exp['target'])\n        if key not in transfer_by_target:\n            transfer_by_target[key] = []\n        transfer_by_target[key].append(exp)\n    \n    for (algo, target_game), transfer_exps in sorted(transfer_by_target.items()):\n        # Check if we have a baseline for this algorithm and target game\n        baseline_key = (algo, target_game)\n        if baseline_key not in baseline_experiments:\n            print(f\"Skipping {algo}/{target_game}: No baseline found\")\n            continue\n        \n        baseline_exp = baseline_experiments[baseline_key]\n        \n        # Load baseline data\n        baseline_data = load_tensorboard_data(baseline_exp['logs'], metric)\n        if len(baseline_data) == 0:\n            print(f\"Skipping {algo}/{target_game}: No baseline data\")\n            continue\n        \n        # Create figure with 3 subplots\n        fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n        fig.suptitle(f'{algo.upper()}: Transfer vs Baseline on {target_game}', \n                    fontsize=14, fontweight='bold')\n        \n        # Subplot 1: Learning curves comparison\n        ax1 = axes[0]\n        \n        # Plot baseline\n        ax1.plot(baseline_data['step'], baseline_data['value'], \n                label='Baseline (from scratch)', \n                color='gray', linewidth=2.5, linestyle='--', alpha=0.8)\n        \n        # Plot transfer experiments\n        colors = plt.cm.tab10(np.linspace(0, 1, len(transfer_exps)))\n        for i, exp in enumerate(transfer_exps):\n            if exp['target_logs'] is None or not os.path.exists(exp['target_logs']):\n                continue\n            \n            data = load_tensorboard_data(exp['target_logs'], metric)\n            if len(data) > 0:\n                label = f\"Transfer: {exp['source']} \u2192 {target_game}\"\n                ax1.plot(data['step'], data['value'], \n                        label=label, color=colors[i], linewidth=2, alpha=0.8)\n        \n        ax1.set_xlabel('Timesteps', fontsize=11)\n        ax1.set_ylabel('Episode Reward', fontsize=11)\n        ax1.set_title('Learning Curves', fontsize=12)\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n        \n        # Subplot 2: Improvement over baseline (absolute difference)\n        ax2 = axes[1]\n        \n        # Collect all improvement curves for averaging\n        all_improvements = []\n        all_common_steps = []\n        \n        for i, exp in enumerate(transfer_exps):\n            if exp['target_logs'] is None or not os.path.exists(exp['target_logs']):\n                continue\n            \n            transfer_data = load_tensorboard_data(exp['target_logs'], metric)\n            if len(transfer_data) == 0:\n                continue\n            \n            # Interpolate both curves to common timesteps\n            common_steps = sorted(set(baseline_data['step']).intersection(set(transfer_data['step'])))\n            if len(common_steps) == 0:\n                # Use all steps and interpolate\n                all_steps = sorted(set(list(baseline_data['step']) + list(transfer_data['step'])))\n                baseline_interp = np.interp(all_steps, baseline_data['step'], baseline_data['value'])\n                transfer_interp = np.interp(all_steps, transfer_data['step'], transfer_data['value'])\n                common_steps = all_steps\n            else:\n                baseline_interp = np.interp(common_steps, baseline_data['step'], baseline_data['value'])\n                transfer_interp = np.interp(common_steps, transfer_data['step'], transfer_data['value'])\n            \n            # Calculate improvement (positive = better than baseline)\n            improvement = transfer_interp - baseline_interp\n            \n            # Plot individual curve (more transparent)\n            label = f\"{exp['source']} \u2192 {target_game}\"\n            ax2.plot(common_steps, improvement, \n                    label=label, color=colors[i], linewidth=1.5, alpha=0.4)\n            \n            # Store for averaging\n            all_improvements.append(improvement)\n            all_common_steps.append(common_steps)\n        \n        # Compute and plot average improvement\n        if len(all_improvements) > 1:\n            # Find common timesteps across all experiments\n            min_len = min(len(steps) for steps in all_common_steps)\n            # Truncate all to same length and compute mean\n            truncated_improvements = [imp[:min_len] for imp in all_improvements]\n            avg_improvement = np.mean(truncated_improvements, axis=0)\n            avg_steps = all_common_steps[0][:min_len]\n            \n            # Smooth the average\n            smoothed_avg = smooth_curve(avg_improvement, window_size=smooth_window)\n            \n            ax2.plot(avg_steps, smoothed_avg, \n                    label='Average (smoothed)', \n                    color='black', linewidth=3, linestyle='-', alpha=0.9, zorder=100)\n        \n        ax2.axhline(y=0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n        ax2.set_xlabel('Timesteps', fontsize=11)\n        ax2.set_ylabel('Reward Improvement vs Baseline', fontsize=11)\n        ax2.set_title('Absolute Improvement (Transfer - Baseline)', fontsize=12)\n        ax2.legend()\n        ax2.grid(True, alpha=0.3)\n        \n        # Subplot 3: Relative improvement (percentage)\n        ax3 = axes[2]\n        \n        # Collect all relative improvement curves for averaging\n        all_rel_improvements = []\n        \n        for i, exp in enumerate(transfer_exps):\n            if exp['target_logs'] is None or not os.path.exists(exp['target_logs']):\n                continue\n            \n            transfer_data = load_tensorboard_data(exp['target_logs'], metric)\n            if len(transfer_data) == 0:\n                continue\n            \n            # Interpolate both curves to common timesteps\n            common_steps = sorted(set(baseline_data['step']).intersection(set(transfer_data['step'])))\n            if len(common_steps) == 0:\n                all_steps = sorted(set(list(baseline_data['step']) + list(transfer_data['step'])))\n                baseline_interp = np.interp(all_steps, baseline_data['step'], baseline_data['value'])\n                transfer_interp = np.interp(all_steps, transfer_data['step'], transfer_data['value'])\n                common_steps = all_steps\n            else:\n                baseline_interp = np.interp(common_steps, baseline_data['step'], baseline_data['value'])\n                transfer_interp = np.interp(common_steps, transfer_data['step'], transfer_data['value'])\n            \n            # Calculate relative improvement (handle division by zero)\n            with np.errstate(divide='ignore', invalid='ignore'):\n                relative_improvement = np.where(\n                    np.abs(baseline_interp) > 1e-6,\n                    ((transfer_interp - baseline_interp) / np.abs(baseline_interp)) * 100,\n                    0\n                )\n            \n            # Plot individual curve (more transparent)\n            label = f\"{exp['source']} \u2192 {target_game}\"\n            ax3.plot(common_steps, relative_improvement, \n                    label=label, color=colors[i], linewidth=1.5, alpha=0.4)\n            \n            # Store for averaging\n            all_rel_improvements.append(relative_improvement)\n        \n        # Compute and plot average relative improvement\n        if len(all_rel_improvements) > 1:\n            # Find common length\n            min_len = min(len(imp) for imp in all_rel_improvements)\n            truncated_rel_improvements = [imp[:min_len] for imp in all_rel_improvements]\n            avg_rel_improvement = np.mean(truncated_rel_improvements, axis=0)\n            avg_steps = all_common_steps[0][:min_len]\n            \n            # Smooth the average\n            smoothed_avg_rel = smooth_curve(avg_rel_improvement, window_size=smooth_window)\n            \n            ax3.plot(avg_steps, smoothed_avg_rel, \n                    label='Average (smoothed)', \n                    color='black', linewidth=3, linestyle='-', alpha=0.9, zorder=100)\n        \n        ax3.axhline(y=0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n        ax3.set_xlabel('Timesteps', fontsize=11)\n        ax3.set_ylabel('Relative Improvement (%)', fontsize=11)\n        ax3.set_title('Relative Improvement ((Transfer - Baseline) / |Baseline| \u00d7 100)', fontsize=12)\n        ax3.legend()\n        ax3.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.savefig(os.path.join(OUTPUT_DIR, f'transfer_vs_baseline_{algo}_{target_game}.png'), \n                   dpi=300, bbox_inches='tight')\n        plt.show()\n\n# Plot transfer vs baseline comparisons\nprint(\"\\n\" + \"=\"*80)\nprint(\"TRANSFER VS BASELINE COMPARISON\")\nprint(\"=\"*80)\nprint(f\"Smoothing window size: 10 (adjustable via smooth_window parameter)\")\nplot_transfer_vs_baseline(experiments, baseline_experiments, smooth_window=10)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transfer Benefit Analysis\n",
    "\n",
    "Compute transfer benefit as the improvement over from-scratch baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_final_performance(log_dir, metric=\"rollout/ep_rew_mean\", last_n_steps=50000):\n",
    "    \"\"\"\n",
    "    Compute average performance over the last N steps.\n",
    "    \"\"\"\n",
    "    data = load_tensorboard_data(log_dir, metric)\n",
    "    if len(data) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Get data from last N steps\n",
    "    max_step = data['step'].max()\n",
    "    final_data = data[data['step'] >= (max_step - last_n_steps)]\n",
    "    \n",
    "    if len(final_data) == 0:\n",
    "        return data['value'].mean()\n",
    "    \n",
    "    return final_data['value'].mean()\n",
    "\n",
    "\n",
    "def compute_transfer_benefits(experiments):\n",
    "    \"\"\"\n",
    "    Compute transfer benefit for each experiment.\n",
    "    \n",
    "    Transfer benefit = (target performance - baseline) / |baseline| * 100\n",
    "    where baseline is the source game's from-scratch performance.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for exp in experiments:\n",
    "        # Get target performance (with transfer)\n",
    "        if exp['target_logs'] is None or not os.path.exists(exp['target_logs']):\n",
    "            continue\n",
    "        \n",
    "        target_perf = compute_final_performance(exp['target_logs'])\n",
    "        if target_perf is None:\n",
    "            continue\n",
    "        \n",
    "        # Get baseline performance (source game from scratch)\n",
    "        # This is the source_logs of the same experiment\n",
    "        baseline_perf = None\n",
    "        if exp['source_logs'] and os.path.exists(exp['source_logs']):\n",
    "            baseline_perf = compute_final_performance(exp['source_logs'])\n",
    "        \n",
    "        if baseline_perf is None or baseline_perf == 0:\n",
    "            continue\n",
    "        \n",
    "        # Compute transfer benefit\n",
    "        benefit = ((target_perf - baseline_perf) / abs(baseline_perf)) * 100\n",
    "        \n",
    "        results.append({\n",
    "            'algorithm': exp['algorithm'],\n",
    "            'source': exp['source'],\n",
    "            'target': exp['target'],\n",
    "            'pretrained': exp['pretrained'],\n",
    "            'baseline_performance': baseline_perf,\n",
    "            'target_performance': target_perf,\n",
    "            'transfer_benefit_pct': benefit\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Compute transfer benefits\n",
    "benefits_df = compute_transfer_benefits(experiments)\n",
    "\n",
    "if len(benefits_df) > 0:\n",
    "    print(\"\\nTransfer Benefit Analysis:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(benefits_df.to_string(index=False))\n",
    "    \n",
    "    # Save to CSV\n",
    "    benefits_df.to_csv(os.path.join(OUTPUT_DIR, 'transfer_benefits.csv'), index=False)\n",
    "    print(f\"\\nSaved to {os.path.join(OUTPUT_DIR, 'transfer_benefits.csv')}\")\n",
    "else:\n",
    "    print(\"No transfer benefit data available yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transfer Benefit Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(benefits_df) > 0:\n",
    "    # Plot transfer benefits by algorithm\n",
    "    algorithms = benefits_df['algorithm'].unique()\n",
    "    \n",
    "    for algo in algorithms:\n",
    "        algo_df = benefits_df[benefits_df['algorithm'] == algo]\n",
    "        \n",
    "        if len(algo_df) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Create transfer pair labels\n",
    "        algo_df = algo_df.copy()\n",
    "        algo_df['transfer_pair'] = algo_df['source'] + ' \u2192 ' + algo_df['target']\n",
    "        \n",
    "        # Sort by benefit\n",
    "        algo_df = algo_df.sort_values('transfer_benefit_pct')\n",
    "        \n",
    "        # Plot\n",
    "        fig, ax = plt.subplots(figsize=(12, max(6, len(algo_df) * 0.4)))\n",
    "        \n",
    "        colors = ['green' if x > 0 else 'red' for x in algo_df['transfer_benefit_pct']]\n",
    "        bars = ax.barh(algo_df['transfer_pair'], algo_df['transfer_benefit_pct'], color=colors, alpha=0.7)\n",
    "        \n",
    "        ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "        ax.set_xlabel('Transfer Benefit (%)', fontsize=12)\n",
    "        ax.set_ylabel('Transfer Pair', fontsize=12)\n",
    "        ax.set_title(f'{algo.upper()} Transfer Benefit Analysis', fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (idx, row) in enumerate(algo_df.iterrows()):\n",
    "            value = row['transfer_benefit_pct']\n",
    "            x_pos = value + (5 if value > 0 else -5)\n",
    "            ha = 'left' if value > 0 else 'right'\n",
    "            ax.text(x_pos, i, f'{value:.1f}%', ha=ha, va='center', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f'transfer_benefit_{algo}.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRANSFER BENEFIT SUMMARY BY ALGORITHM\")\n",
    "    print(\"=\"*80)\n",
    "    summary = benefits_df.groupby('algorithm')['transfer_benefit_pct'].agg(['mean', 'std', 'min', 'max', 'count'])\n",
    "    summary.columns = ['Mean (%)', 'Std (%)', 'Min (%)', 'Max (%)', 'N']\n",
    "    print(summary.to_string())\n",
    "else:\n",
    "    print(\"No transfer benefit data to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transfer Matrix Heatmap\n",
    "\n",
    "Create a heatmap showing transfer benefits between all game pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(benefits_df) > 0:\n",
    "    algorithms = benefits_df['algorithm'].unique()\n",
    "    \n",
    "    for algo in algorithms:\n",
    "        algo_df = benefits_df[benefits_df['algorithm'] == algo]\n",
    "        \n",
    "        if len(algo_df) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Create pivot table for heatmap\n",
    "        games = sorted(set(algo_df['source'].tolist() + algo_df['target'].tolist()))\n",
    "        matrix = pd.DataFrame(index=games, columns=games, dtype=float)\n",
    "        \n",
    "        for _, row in algo_df.iterrows():\n",
    "            matrix.loc[row['source'], row['target']] = row['transfer_benefit_pct']\n",
    "        \n",
    "        # Plot heatmap\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        \n",
    "        sns.heatmap(matrix, annot=True, fmt=\".1f\", cmap=\"RdYlGn\", center=0,\n",
    "                   cbar_kws={'label': 'Transfer Benefit (%)'}, ax=ax,\n",
    "                   linewidths=0.5, linecolor='gray')\n",
    "        \n",
    "        ax.set_xlabel('Target Game', fontsize=12)\n",
    "        ax.set_ylabel('Source Game', fontsize=12)\n",
    "        ax.set_title(f'{algo.upper()} Transfer Learning Matrix\\n(Source \u2192 Target)', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f'transfer_matrix_{algo}.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No data available for transfer matrix.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Pretrained vs From-Scratch Source\n",
    "\n",
    "If you have both pretrained and from-scratch experiments, compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(benefits_df) > 0 and 'pretrained' in benefits_df.columns:\n",
    "    # Check if we have both pretrained and non-pretrained experiments\n",
    "    has_pretrained = benefits_df['pretrained'].any()\n",
    "    has_scratch = (~benefits_df['pretrained']).any()\n",
    "    \n",
    "    if has_pretrained and has_scratch:\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        for algo in benefits_df['algorithm'].unique():\n",
    "            algo_df = benefits_df[benefits_df['algorithm'] == algo]\n",
    "            \n",
    "            pretrained_mean = algo_df[algo_df['pretrained']]['transfer_benefit_pct'].mean()\n",
    "            scratch_mean = algo_df[~algo_df['pretrained']]['transfer_benefit_pct'].mean()\n",
    "            \n",
    "            x = np.arange(2)\n",
    "            width = 0.25\n",
    "            offset = list(benefits_df['algorithm'].unique()).index(algo) * width\n",
    "            \n",
    "            ax.bar(x + offset, [scratch_mean, pretrained_mean], width, \n",
    "                  label=algo.upper(), alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Source Model Type', fontsize=12)\n",
    "        ax.set_ylabel('Average Transfer Benefit (%)', fontsize=12)\n",
    "        ax.set_title('Pretrained vs From-Scratch Source Models', fontsize=14, fontweight='bold')\n",
    "        ax.set_xticks([0.25, 1.25])\n",
    "        ax.set_xticklabels(['From Scratch', 'Pretrained (Zoo)'])\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        ax.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, 'pretrained_vs_scratch.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Need both pretrained and from-scratch experiments for comparison.\")\n",
    "else:\n",
    "    print(\"No pretrained comparison data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "All plots have been saved to the `analysis_plots/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal experiments analyzed: {len(experiments)}\")\n",
    "print(f\"Algorithms: {', '.join(sorted(set(exp['algorithm'] for exp in experiments)))}\")\n",
    "print(f\"\\nPlots saved to: {OUTPUT_DIR}/\")\n",
    "print(\"\\nGenerated files:\")\n",
    "for f in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Configuration for phased experiments\nPHASED_DIR = \"results_phased\"  # or \"/mnt/home/slee1/ceph/atari_phased_results\"\n\nprint(f\"Phased experiments directory: {PHASED_DIR}\")\nprint(f\"Looking for experiments in: {os.path.abspath(PHASED_DIR) if os.path.exists(PHASED_DIR) else 'NOT FOUND'}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Configuration for phased experiments\n",
    "PHASED_DIR = \"results_phased\"  # or \"/mnt/home/slee1/ceph/atari_phased_results\"\n",
    "\n",
    "print(f\"Phased experiments directory: {PHASED_DIR}\")\n",
    "print(f\"Looking for experiments in: {os.path.abspath(PHASED_DIR) if os.path.exists(PHASED_DIR) else 'NOT FOUND'}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def find_phased_experiments(phased_dir):\n",
    "    \"\"\"\n",
    "    Find phased transfer learning experiments.\n",
    "    \n",
    "    Returns:\n",
    "        source_experiments: Dict of source training experiments\n",
    "            key: (algorithm, source_game)\n",
    "            value: {path, checkpoints, logs, phases}\n",
    "        transfer_experiments: List of transfer experiments\n",
    "            [{algorithm, source, target, phase, checkpoint, path, logs}, ...]\n",
    "    \"\"\"\n",
    "    if not os.path.exists(phased_dir):\n",
    "        print(f\"Phased directory not found: {phased_dir}\")\n",
    "        return {}, []\n",
    "    \n",
    "    source_experiments = {}\n",
    "    transfer_experiments = []\n",
    "    \n",
    "    for exp_name in os.listdir(phased_dir):\n",
    "        exp_path = os.path.join(phased_dir, exp_name)\n",
    "        \n",
    "        if not os.path.isdir(exp_path):\n",
    "            continue\n",
    "        \n",
    "        # Skip script directories\n",
    "        if 'slurm' in exp_name:\n",
    "            continue\n",
    "        \n",
    "        # Parse source experiments: {algorithm}_{game}_source\n",
    "        if exp_name.endswith('_source'):\n",
    "            parts = exp_name.split('_')\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            \n",
    "            algorithm = parts[0]\n",
    "            source_game = parts[1]\n",
    "            \n",
    "            # Find checkpoints\n",
    "            checkpoint_dir = os.path.join(exp_path, 'checkpoints')\n",
    "            logs_dir = os.path.join(exp_path, 'logs')\n",
    "            \n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                continue\n",
    "            \n",
    "            # Find phase checkpoints\n",
    "            phase_checkpoints = {}\n",
    "            for ckpt_file in os.listdir(checkpoint_dir):\n",
    "                if ckpt_file.startswith('phase_') and ckpt_file.endswith('_model.zip'):\n",
    "                    # Extract phase number\n",
    "                    phase_str = ckpt_file.replace('phase_', '').replace('_model.zip', '')\n",
    "                    try:\n",
    "                        phase_num = int(phase_str)\n",
    "                        phase_checkpoints[phase_num] = os.path.join(checkpoint_dir, ckpt_file)\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "            \n",
    "            key = (algorithm, source_game)\n",
    "            source_experiments[key] = {\n",
    "                'algorithm': algorithm,\n",
    "                'source_game': source_game,\n",
    "                'path': exp_path,\n",
    "                'checkpoints': phase_checkpoints,\n",
    "                'logs': logs_dir if os.path.exists(logs_dir) else None,\n",
    "                'num_phases': len(phase_checkpoints)\n",
    "            }\n",
    "        \n",
    "        # Parse transfer experiments: {algorithm}_{source}_to_{target}_from_{checkpoint}_{jobid}\n",
    "        elif '_to_' in exp_name and '_from_' in exp_name:\n",
    "            parts = exp_name.split('_')\n",
    "            \n",
    "            try:\n",
    "                to_idx = parts.index('to')\n",
    "                from_idx = parts.index('from')\n",
    "                \n",
    "                algorithm = parts[0]\n",
    "                source_game = parts[to_idx - 1]\n",
    "                target_game = parts[to_idx + 1]\n",
    "                \n",
    "                # Extract phase from checkpoint name\n",
    "                # Checkpoint name format: phase_X_model\n",
    "                checkpoint_parts = parts[from_idx + 1:]\n",
    "                if len(checkpoint_parts) >= 2 and checkpoint_parts[0] == 'phase':\n",
    "                    try:\n",
    "                        phase_num = int(checkpoint_parts[1])\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                # Find logs\n",
    "                logs_dir = os.path.join(exp_path, 'logs')\n",
    "                \n",
    "                transfer_experiments.append({\n",
    "                    'algorithm': algorithm,\n",
    "                    'source': source_game,\n",
    "                    'target': target_game,\n",
    "                    'phase': phase_num,\n",
    "                    'checkpoint_name': '_'.join(checkpoint_parts[:3]),  # phase_X_model\n",
    "                    'path': exp_path,\n",
    "                    'logs': logs_dir if os.path.exists(logs_dir) else None,\n",
    "                    'name': exp_name\n",
    "                })\n",
    "            except (ValueError, IndexError):\n",
    "                continue\n",
    "    \n",
    "    return source_experiments, transfer_experiments\n",
    "\n",
    "\n",
    "# Load phased experiments\n",
    "phased_sources, phased_transfers = find_phased_experiments(PHASED_DIR)\n",
    "\n",
    "print(f\"\\nFound {len(phased_sources)} source training experiments\")\n",
    "print(f\"Found {len(phased_transfers)} transfer experiments\\n\")\n",
    "\n",
    "if phased_sources:\n",
    "    print(\"Source experiments:\")\n",
    "    for (algo, game), exp in sorted(phased_sources.items()):\n",
    "        print(f\"  {algo.upper()}/{game}: {exp['num_phases']} phases\")\n",
    "\n",
    "if phased_transfers:\n",
    "    print(f\"\\nTransfer experiments by phase:\")\n",
    "    transfer_df = pd.DataFrame(phased_transfers)\n",
    "    phase_summary = transfer_df.groupby(['algorithm', 'phase']).size().unstack(fill_value=0)\n",
    "    print(phase_summary)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phased Analysis: Transfer Performance by Phase\n",
    "\n",
    "For each phase checkpoint, compare baseline vs transfer performance."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_phased_transfer_comparison(phased_sources, phased_transfers, baseline_experiments,\n",
    "                                    metric=\"rollout/ep_rew_mean\", smooth_window=10):\n",
    "    \"\"\"\n",
    "    Plot transfer performance by phase compared to baseline.\n",
    "    \n",
    "    For each target game:\n",
    "    - Show baseline (training from scratch)\n",
    "    - Show transfer from each source at each phase\n",
    "    - Compare how performance changes with more source training\n",
    "    \"\"\"\n",
    "    # Group transfers by algorithm and target\n",
    "    transfers_by_target = {}\n",
    "    for transfer in phased_transfers:\n",
    "        key = (transfer['algorithm'], transfer['target'])\n",
    "        if key not in transfers_by_target:\n",
    "            transfers_by_target[key] = []\n",
    "        transfers_by_target[key].append(transfer)\n",
    "    \n",
    "    for (algo, target_game), transfers in sorted(transfers_by_target.items()):\n",
    "        # Check for baseline\n",
    "        baseline_key = (algo, target_game)\n",
    "        if baseline_key not in baseline_experiments:\n",
    "            print(f\"Skipping {algo}/{target_game}: No baseline found\")\n",
    "            continue\n",
    "        \n",
    "        baseline_exp = baseline_experiments[baseline_key]\n",
    "        baseline_data = load_tensorboard_data(baseline_exp['logs'], metric)\n",
    "        \n",
    "        if len(baseline_data) == 0:\n",
    "            print(f\"Skipping {algo}/{target_game}: No baseline data\")\n",
    "            continue\n",
    "        \n",
    "        # Group transfers by source and phase\n",
    "        transfers_by_source = {}\n",
    "        for transfer in transfers:\n",
    "            source = transfer['source']\n",
    "            if source not in transfers_by_source:\n",
    "                transfers_by_source[source] = {}\n",
    "            phase = transfer['phase']\n",
    "            transfers_by_source[source][phase] = transfer\n",
    "        \n",
    "        # Create figure - one subplot per source game\n",
    "        num_sources = len(transfers_by_source)\n",
    "        fig, axes = plt.subplots(1, num_sources, figsize=(8 * num_sources, 6))\n",
    "        if num_sources == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        fig.suptitle(f'{algo.upper()}: Transfer to {target_game} by Phase',\n",
    "                    fontsize=14, fontweight='bold')\n",
    "        \n",
    "        for ax, (source_game, phase_transfers) in zip(axes, sorted(transfers_by_source.items())):\n",
    "            # Plot baseline\n",
    "            ax.plot(baseline_data['step'], baseline_data['value'],\n",
    "                   label='Baseline (from scratch)',\n",
    "                   color='gray', linewidth=2.5, linestyle='--', alpha=0.8, zorder=100)\n",
    "            \n",
    "            # Plot each phase transfer\n",
    "            phases = sorted(phase_transfers.keys())\n",
    "            colors = plt.cm.viridis(np.linspace(0, 1, len(phases)))\n",
    "            \n",
    "            for i, phase in enumerate(phases):\n",
    "                transfer = phase_transfers[phase]\n",
    "                if transfer['logs'] is None or not os.path.exists(transfer['logs']):\n",
    "                    continue\n",
    "                \n",
    "                data = load_tensorboard_data(transfer['logs'], metric)\n",
    "                if len(data) == 0:\n",
    "                    continue\n",
    "                \n",
    "                label = f\"Phase {phase}\"\n",
    "                ax.plot(data['step'], data['value'],\n",
    "                       label=label, color=colors[i], linewidth=2, alpha=0.7)\n",
    "            \n",
    "            ax.set_xlabel('Timesteps', fontsize=11)\n",
    "            ax.set_ylabel('Episode Reward', fontsize=11)\n",
    "            ax.set_title(f'Source: {source_game}', fontsize=12)\n",
    "            ax.legend(loc='best')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f'phased_transfer_{algo}_{target_game}.png'),\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if phased_transfers and baseline_experiments:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PHASED TRANSFER ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    plot_phased_transfer_comparison(phased_sources, phased_transfers, baseline_experiments)\n",
    "else:\n",
    "    print(\"Need both phased transfers and baseline experiments for comparison.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Performance by Phase\n",
    "\n",
    "Analyze how final transfer performance changes with more source training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def analyze_performance_by_phase(phased_transfers, baseline_experiments,\n",
    "                                metric=\"rollout/ep_rew_mean\", last_n_steps=50000):\n",
    "    \"\"\"\n",
    "    Analyze how transfer performance changes with phase.\n",
    "    \n",
    "    Returns DataFrame with columns:\n",
    "    - algorithm, source, target, phase\n",
    "    - baseline_perf, transfer_perf, improvement, improvement_pct\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for transfer in phased_transfers:\n",
    "        if transfer['logs'] is None or not os.path.exists(transfer['logs']):\n",
    "            continue\n",
    "        \n",
    "        # Get transfer performance\n",
    "        transfer_perf = compute_final_performance(transfer['logs'], metric, last_n_steps)\n",
    "        if transfer_perf is None:\n",
    "            continue\n",
    "        \n",
    "        # Get baseline performance\n",
    "        baseline_key = (transfer['algorithm'], transfer['target'])\n",
    "        if baseline_key not in baseline_experiments:\n",
    "            continue\n",
    "        \n",
    "        baseline_exp = baseline_experiments[baseline_key]\n",
    "        baseline_perf = compute_final_performance(baseline_exp['logs'], metric, last_n_steps)\n",
    "        \n",
    "        if baseline_perf is None or abs(baseline_perf) < 1e-6:\n",
    "            continue\n",
    "        \n",
    "        # Compute improvement\n",
    "        improvement = transfer_perf - baseline_perf\n",
    "        improvement_pct = (improvement / abs(baseline_perf)) * 100\n",
    "        \n",
    "        results.append({\n",
    "            'algorithm': transfer['algorithm'],\n",
    "            'source': transfer['source'],\n",
    "            'target': transfer['target'],\n",
    "            'phase': transfer['phase'],\n",
    "            'baseline_perf': baseline_perf,\n",
    "            'transfer_perf': transfer_perf,\n",
    "            'improvement': improvement,\n",
    "            'improvement_pct': improvement_pct\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Analyze performance by phase\n",
    "if phased_transfers and baseline_experiments:\n",
    "    phased_results = analyze_performance_by_phase(phased_transfers, baseline_experiments)\n",
    "    \n",
    "    if len(phased_results) > 0:\n",
    "        print(\"\\nPerformance by Phase:\")\n",
    "        print(\"=\"*80)\n",
    "        print(phased_results.to_string(index=False))\n",
    "        \n",
    "        # Save to CSV\n",
    "        phased_results.to_csv(os.path.join(OUTPUT_DIR, 'phased_transfer_results.csv'), index=False)\n",
    "        print(f\"\\nSaved to {os.path.join(OUTPUT_DIR, 'phased_transfer_results.csv')}\")\n",
    "    else:\n",
    "        print(\"No phased results data available yet.\")\n",
    "else:\n",
    "    print(\"Need phased transfers and baseline experiments.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase Progression Plot\n",
    "\n",
    "Show how transfer benefit changes with more source training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if 'phased_results' in locals() and len(phased_results) > 0:\n",
    "    # Group by algorithm\n",
    "    for algo in phased_results['algorithm'].unique():\n",
    "        algo_df = phased_results[phased_results['algorithm'] == algo]\n",
    "        \n",
    "        # Group by source-target pair\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        fig.suptitle(f'{algo.upper()}: Transfer Performance vs Source Training Phase',\n",
    "                    fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Plot 1: Absolute improvement\n",
    "        ax1 = axes[0]\n",
    "        for (source, target), group in algo_df.groupby(['source', 'target']):\n",
    "            group = group.sort_values('phase')\n",
    "            label = f\"{source} \u2192 {target}\"\n",
    "            ax1.plot(group['phase'], group['improvement'],\n",
    "                    marker='o', label=label, linewidth=2, markersize=8)\n",
    "        \n",
    "        ax1.axhline(y=0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "        ax1.set_xlabel('Source Training Phase', fontsize=11)\n",
    "        ax1.set_ylabel('Improvement vs Baseline (Reward)', fontsize=11)\n",
    "        ax1.set_title('Absolute Improvement', fontsize=12)\n",
    "        ax1.legend(loc='best')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Relative improvement\n",
    "        ax2 = axes[1]\n",
    "        for (source, target), group in algo_df.groupby(['source', 'target']):\n",
    "            group = group.sort_values('phase')\n",
    "            label = f\"{source} \u2192 {target}\"\n",
    "            ax2.plot(group['phase'], group['improvement_pct'],\n",
    "                    marker='o', label=label, linewidth=2, markersize=8)\n",
    "        \n",
    "        ax2.axhline(y=0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "        ax2.set_xlabel('Source Training Phase', fontsize=11)\n",
    "        ax2.set_ylabel('Improvement vs Baseline (%)', fontsize=11)\n",
    "        ax2.set_title('Relative Improvement', fontsize=12)\n",
    "        ax2.legend(loc='best')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f'phased_progression_{algo}.png'),\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(f\"\\n{algo.upper()} - Summary by Phase:\")\n",
    "        print(\"=\"*80)\n",
    "        phase_summary = algo_df.groupby('phase').agg({\n",
    "            'improvement_pct': ['mean', 'std', 'min', 'max'],\n",
    "            'transfer_perf': 'mean'\n",
    "        })\n",
    "        phase_summary.columns = ['Mean Improvement (%)', 'Std (%)', 'Min (%)', 'Max (%)', 'Mean Transfer Perf']\n",
    "        print(phase_summary.to_string())\n",
    "else:\n",
    "    print(\"No phased results available for progression plot.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap: Transfer Benefit by Phase\n",
    "\n",
    "Visualize how different source games transfer to targets at each phase."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if 'phased_results' in locals() and len(phased_results) > 0:\n",
    "    for algo in phased_results['algorithm'].unique():\n",
    "        algo_df = phased_results[phased_results['algorithm'] == algo]\n",
    "        \n",
    "        # Get unique phases\n",
    "        phases = sorted(algo_df['phase'].unique())\n",
    "        \n",
    "        # Create subplots - one per phase\n",
    "        num_phases = len(phases)\n",
    "        cols = min(3, num_phases)\n",
    "        rows = (num_phases + cols - 1) // cols\n",
    "        \n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(7 * cols, 6 * rows))\n",
    "        if num_phases == 1:\n",
    "            axes = np.array([axes])\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        fig.suptitle(f'{algo.upper()}: Transfer Benefit Heatmap by Phase',\n",
    "                    fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for i, phase in enumerate(phases):\n",
    "            phase_df = algo_df[algo_df['phase'] == phase]\n",
    "            \n",
    "            # Create pivot table\n",
    "            sources = sorted(phase_df['source'].unique())\n",
    "            targets = sorted(phase_df['target'].unique())\n",
    "            \n",
    "            matrix = pd.DataFrame(index=sources, columns=targets, dtype=float)\n",
    "            for _, row in phase_df.iterrows():\n",
    "                matrix.loc[row['source'], row['target']] = row['improvement_pct']\n",
    "            \n",
    "            # Plot heatmap\n",
    "            ax = axes[i]\n",
    "            sns.heatmap(matrix, annot=True, fmt=\".1f\", cmap=\"RdYlGn\", center=0,\n",
    "                       cbar_kws={'label': 'Improvement (%)'},\n",
    "                       linewidths=0.5, linecolor='gray', ax=ax)\n",
    "            ax.set_xlabel('Target Game', fontsize=10)\n",
    "            ax.set_ylabel('Source Game', fontsize=10)\n",
    "            ax.set_title(f'Phase {phase}', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Hide extra subplots\n",
    "        for i in range(num_phases, len(axes)):\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f'phased_heatmap_{algo}.png'),\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No phased results for heatmap.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phased Analysis Summary\n",
    "\n",
    "All phased experiment plots have been saved."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASED ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'phased_results' in locals() and len(phased_results) > 0:\n",
    "    print(f\"\\nTotal phased experiments analyzed: {len(phased_results)}\")\n",
    "    print(f\"Algorithms: {', '.join(sorted(phased_results['algorithm'].unique()))}\")\n",
    "    print(f\"Phases: {sorted(phased_results['phase'].unique())}\")\n",
    "    print(f\"\\nPhased plots saved to: {OUTPUT_DIR}/\")\n",
    "    \n",
    "    phased_files = [f for f in os.listdir(OUTPUT_DIR) if 'phased' in f]\n",
    "    if phased_files:\n",
    "        print(\"\\nGenerated phased analysis files:\")\n",
    "        for f in sorted(phased_files):\n",
    "            print(f\"  - {f}\")\n",
    "else:\n",
    "    print(\"\\nNo phased experiments found or not yet complete.\")\n",
    "    print(f\"Make sure {PHASED_DIR} contains completed phased experiments.\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}