{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning Analysis for Atari Games\n",
    "\n",
    "This notebook analyzes transfer learning results including:\n",
    "1. Learning curves for each experiment\n",
    "2. Transfer benefit analysis (comparing transfer vs from-scratch performance)\n",
    "3. Visualizations split by algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Define the results directory and experiment parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "RESULTS_DIR = \"results\"  # Transfer learning results\n",
    "BASELINE_DIR = \"results_baseline\"  # Baseline (from-scratch) results\n",
    "OUTPUT_DIR = \"analysis_plots\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Games and algorithms from your config\n",
    "GAMES = [\"Pong\", \"Breakout\", \"SpaceInvaders\", \"Tennis\"]\n",
    "ALGORITHMS = [\"dqn\", \"ppo\", \"qrdqn\"]\n",
    "\n",
    "# Metrics to extract from TensorBoard\n",
    "METRICS = [\n",
    "    \"eval/mean_reward\",  # Evaluation reward (from eval callback)\n",
    "    \"rollout/ep_len_mean\",  # Episode length\n",
    "    \"train/loss\",            # Training loss\n",
    "]\n",
    "\n",
    "print(f\"Transfer results directory: {RESULTS_DIR}\")\n",
    "print(f\"Baseline results directory: {BASELINE_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Plots will be saved to: {os.path.abspath(OUTPUT_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tensorboard_data(log_dir, metric=\"eval/mean_reward\", fix_resets=True):\n",
    "    \"\"\"\n",
    "    Load data from TensorBoard event files.\n",
    "    \n",
    "    Args:\n",
    "        log_dir: Path to TensorBoard log directory\n",
    "        metric: Metric name to extract\n",
    "        fix_resets: If True, detect and fix step counter resets (for phased training)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: step, value\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ea = event_accumulator.EventAccumulator(log_dir)\n",
    "        ea.Reload()\n",
    "        \n",
    "        if metric not in ea.Tags()['scalars']:\n",
    "            print(f\"Warning: Metric '{metric}' not found in {log_dir}\")\n",
    "            return pd.DataFrame(columns=['step', 'value'])\n",
    "        \n",
    "        events = ea.Scalars(metric)\n",
    "        data = pd.DataFrame([\n",
    "            {'step': e.step, 'value': e.value, 'wall_time': e.wall_time}\n",
    "            for e in events\n",
    "        ])\n",
    "        \n",
    "        if len(data) == 0:\n",
    "            return pd.DataFrame(columns=['step', 'value'])\n",
    "        \n",
    "        # Fix step resets (for phased training where steps reset to 0)\n",
    "        if fix_resets:\n",
    "            # Detect resets: when step decreases or goes to 0\n",
    "            steps = data['step'].values\n",
    "            fixed_steps = [steps[0]]\n",
    "            offset = 0\n",
    "            \n",
    "            for i in range(1, len(steps)):\n",
    "                # Check if step counter reset\n",
    "                if steps[i] < steps[i-1]:\n",
    "                    # Step reset detected - add offset\n",
    "                    offset = fixed_steps[-1]\n",
    "                    print(f\"  Detected step reset at index {i}: {steps[i-1]} -> {steps[i]}, adding offset {offset}\")\n",
    "                \n",
    "                fixed_steps.append(steps[i] + offset)\n",
    "            \n",
    "            data['step'] = fixed_steps\n",
    "        \n",
    "        return data[['step', 'value']]\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {log_dir}: {e}\")\n",
    "        return pd.DataFrame(columns=['step', 'value'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Find all transfer experiments\nexperiments = find_experiments(RESULTS_DIR)\n\nprint(f\"Found {len(experiments)} transfer experiments\")\nprint(\"\\nTransfer experiment summary:\")\nexp_df = pd.DataFrame(experiments)\nif len(exp_df) > 0:\n    print(exp_df[['algorithm', 'source', 'target', 'pretrained']].to_string())\nelse:\n    print(\"No experiments found!\")\n\n# Load baseline experiments\nprint(\"\\n\" + \"=\"*80)\nprint(\"LOADING BASELINE EXPERIMENTS\")\nprint(\"=\"*80)\n\nbaseline_experiments = {}\n\nif os.path.exists(BASELINE_DIR):\n    for exp_name in os.listdir(BASELINE_DIR):\n        exp_path = os.path.join(BASELINE_DIR, exp_name)\n        \n        if not os.path.isdir(exp_path):\n            continue\n        \n        # Parse baseline experiment name: {algorithm}_{game}_baseline_{job_id}\n        parts = exp_name.split('_')\n        if len(parts) < 3 or 'baseline' not in parts:\n            continue\n        \n        algorithm = parts[0]\n        game = parts[1]\n        \n        # Find log directory\n        logs_dir = os.path.join(exp_path, 'logs')\n        if not os.path.exists(logs_dir):\n            continue\n        \n        # Store baseline experiment (keep latest if multiple)\n        key = (algorithm, game)\n        if key not in baseline_experiments or exp_name > baseline_experiments[key]['name']:\n            baseline_experiments[key] = {\n                'algorithm': algorithm,\n                'game': game,\n                'name': exp_name,\n                'path': exp_path,\n                'logs': logs_dir\n            }\n\nprint(f\"\\nFound {len(baseline_experiments)} baseline experiments\")\nif baseline_experiments:\n    print(\"\\nBaseline summary:\")\n    for (algo, game), exp in sorted(baseline_experiments.items()):\n        print(f\"  {algo.upper()}: {game}\")\nelse:\n    print(\"No baseline experiments found. Make sure BASELINE_DIR is correct.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Learning Curves\n",
    "\n",
    "Plot learning curves for each experiment, organized by algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves_by_algorithm(experiments, metric=\"eval/mean_reward\"):\n",
    "    \"\"\"\n",
    "    Plot learning curves grouped by algorithm with consistent colors and average curves.\n",
    "    \"\"\"\n",
    "    algorithms = sorted(set(exp['algorithm'] for exp in experiments))\n",
    "    \n",
    "    # Create consistent color mapping for game combinations\n",
    "    # Get all unique games\n",
    "    all_games = set()\n",
    "    all_pairs = set()\n",
    "    for exp in experiments:\n",
    "        all_games.add(exp['source'])\n",
    "        all_games.add(exp['target'])\n",
    "        all_pairs.add((exp['source'], exp['target']))\n",
    "    \n",
    "    # Create color palettes\n",
    "    game_colors = {}\n",
    "    pair_colors = {}\n",
    "    \n",
    "    # Use a consistent colormap\n",
    "    source_cmap = plt.cm.get_cmap('tab10')\n",
    "    pair_cmap = plt.cm.get_cmap('tab20')\n",
    "    \n",
    "    for i, game in enumerate(sorted(all_games)):\n",
    "        game_colors[game] = source_cmap(i % 10)\n",
    "    \n",
    "    for i, pair in enumerate(sorted(all_pairs)):\n",
    "        pair_colors[pair] = pair_cmap(i % 20)\n",
    "    \n",
    "    for algo in algorithms:\n",
    "        algo_exps = [exp for exp in experiments if exp['algorithm'] == algo]\n",
    "        \n",
    "        if not algo_exps:\n",
    "            continue\n",
    "        \n",
    "        # Create subplots for source and target\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        fig.suptitle(f'{algo.upper()} Learning Curves - {metric}', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Plot source training\n",
    "        ax_source = axes[0]\n",
    "        plotted_sources = set()\n",
    "        source_data_list = []\n",
    "        \n",
    "        for exp in algo_exps:\n",
    "            if exp['source_logs'] is None or not os.path.exists(exp['source_logs']):\n",
    "                continue\n",
    "            \n",
    "            # Avoid duplicate source game plots\n",
    "            if exp['source'] in plotted_sources:\n",
    "                continue\n",
    "            \n",
    "            data = load_tensorboard_data(exp['source_logs'], metric)\n",
    "            if len(data) > 0:\n",
    "                label = f\"{exp['source']}\"\n",
    "                color = game_colors[exp['source']]\n",
    "                ax_source.plot(data['step'], data['value'], label=label, \n",
    "                             color=color, alpha=0.6, linewidth=1.5)\n",
    "                plotted_sources.add(exp['source'])\n",
    "                source_data_list.append(data)\n",
    "        \n",
    "        # Compute and plot average for source\n",
    "        if len(source_data_list) > 1:\n",
    "            # Interpolate all curves to common timesteps\n",
    "            all_steps = sorted(set(step for data in source_data_list for step in data['step']))\n",
    "            interp_values = []\n",
    "            \n",
    "            for data in source_data_list:\n",
    "                interp_val = np.interp(all_steps, data['step'], data['value'])\n",
    "                interp_values.append(interp_val)\n",
    "            \n",
    "            avg_values = np.mean(interp_values, axis=0)\n",
    "            ax_source.plot(all_steps, avg_values, label='Average', \n",
    "                         color='black', linewidth=3, linestyle='--', alpha=0.9, zorder=100)\n",
    "        \n",
    "        ax_source.set_xlabel('Timesteps', fontsize=11)\n",
    "        ax_source.set_ylabel('Reward', fontsize=11)\n",
    "        ax_source.set_title('Source Game Training (from scratch)', fontsize=12)\n",
    "        ax_source.legend()\n",
    "        ax_source.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot target training\n",
    "        ax_target = axes[1]\n",
    "        target_data_list = []\n",
    "        \n",
    "        for exp in algo_exps:\n",
    "            if exp['target_logs'] is None or not os.path.exists(exp['target_logs']):\n",
    "                continue\n",
    "            \n",
    "            data = load_tensorboard_data(exp['target_logs'], metric)\n",
    "            if len(data) > 0:\n",
    "                pretrained_tag = \" (pretrained)\" if exp['pretrained'] else \"\"\n",
    "                label = f\"{exp['source']} \u2192 {exp['target']}{pretrained_tag}\"\n",
    "                color = pair_colors[(exp['source'], exp['target'])]\n",
    "                ax_target.plot(data['step'], data['value'], label=label, \n",
    "                             color=color, alpha=0.6, linewidth=1.5)\n",
    "                target_data_list.append(data)\n",
    "        \n",
    "        # Compute and plot average for target\n",
    "        if len(target_data_list) > 1:\n",
    "            # Interpolate all curves to common timesteps\n",
    "            all_steps = sorted(set(step for data in target_data_list for step in data['step']))\n",
    "            interp_values = []\n",
    "            \n",
    "            for data in target_data_list:\n",
    "                interp_val = np.interp(all_steps, data['step'], data['value'])\n",
    "                interp_values.append(interp_val)\n",
    "            \n",
    "            avg_values = np.mean(interp_values, axis=0)\n",
    "            ax_target.plot(all_steps, avg_values, label='Average', \n",
    "                         color='black', linewidth=3, linestyle='--', alpha=0.9, zorder=100)\n",
    "        \n",
    "        ax_target.set_xlabel('Timesteps', fontsize=11)\n",
    "        ax_target.set_ylabel('Reward', fontsize=11)\n",
    "        ax_target.set_title('Target Game Training (with transfer)', fontsize=12)\n",
    "        ax_target.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        ax_target.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f'learning_curves_{algo}.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "# Plot learning curves\n",
    "plot_learning_curves_by_algorithm(experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Individual Transfer Comparisons\n",
    "\n",
    "For each transfer pair, compare source (baseline) vs target (transfer) performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(data, window_size=10):\n",
    "    \"\"\"\n",
    "    Smooth a curve using a moving average.\n",
    "    \n",
    "    Args:\n",
    "        data: Array of values to smooth\n",
    "        window_size: Size of the smoothing window\n",
    "    \n",
    "    Returns:\n",
    "        Smoothed array\n",
    "    \"\"\"\n",
    "    if len(data) < window_size:\n",
    "        return data\n",
    "    \n",
    "    # Use pandas for efficient rolling mean\n",
    "    smoothed = pd.Series(data).rolling(window=window_size, min_periods=1, center=True).mean()\n",
    "    return smoothed.values\n",
    "\n",
    "\n",
    "def plot_transfer_vs_baseline(experiments, baseline_experiments, metric=\"eval/mean_reward\", \n",
    "                              smooth_window=10):\n",
    "    \"\"\"\n",
    "    Plot transfer learning vs baseline with improvement metric.\n",
    "    \n",
    "    For each transfer experiment, plot:\n",
    "    1. Transfer learning curve (target training with pretrained source)\n",
    "    2. Baseline learning curve (training from scratch)\n",
    "    3. Difference/improvement over time (with average across all transfer experiments)\n",
    "    \n",
    "    Args:\n",
    "        experiments: List of transfer experiments\n",
    "        baseline_experiments: Dict of baseline experiments\n",
    "        metric: Metric to plot\n",
    "        smooth_window: Window size for smoothing the average curve\n",
    "    \"\"\"\n",
    "    # Group by algorithm and target game\n",
    "    transfer_by_target = {}\n",
    "    for exp in experiments:\n",
    "        key = (exp['algorithm'], exp['target'])\n",
    "        if key not in transfer_by_target:\n",
    "            transfer_by_target[key] = []\n",
    "        transfer_by_target[key].append(exp)\n",
    "    \n",
    "    for (algo, target_game), transfer_exps in sorted(transfer_by_target.items()):\n",
    "        # Check if we have a baseline for this algorithm and target game\n",
    "        baseline_key = (algo, target_game)\n",
    "        if baseline_key not in baseline_experiments:\n",
    "            print(f\"Skipping {algo}/{target_game}: No baseline found\")\n",
    "            continue\n",
    "        \n",
    "        baseline_exp = baseline_experiments[baseline_key]\n",
    "        \n",
    "        # Load baseline data\n",
    "        baseline_data = load_tensorboard_data(baseline_exp['logs'], metric)\n",
    "        if len(baseline_data) == 0:\n",
    "            print(f\"Skipping {algo}/{target_game}: No baseline data\")\n",
    "            continue\n",
    "        \n",
    "        # Create figure with 3 subplots\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "        fig.suptitle(f'{algo.upper()}: Transfer vs Baseline on {target_game}', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Subplot 1: Learning curves comparison\n",
    "        ax1 = axes[0]\n",
    "        \n",
    "        # Plot baseline\n",
    "        ax1.plot(baseline_data['step'], baseline_data['value'], \n",
    "                label='Baseline (from scratch)', \n",
    "                color='gray', linewidth=2.5, linestyle='--', alpha=0.8)\n",
    "        \n",
    "        # Plot transfer experiments\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, len(transfer_exps)))\n",
    "        for i, exp in enumerate(transfer_exps):\n",
    "            if exp['target_logs'] is None or not os.path.exists(exp['target_logs']):\n",
    "                continue\n",
    "            \n",
    "            data = load_tensorboard_data(exp['target_logs'], metric)\n",
    "            if len(data) > 0:\n",
    "                label = f\"Transfer: {exp['source']} \u2192 {target_game}\"\n",
    "                ax1.plot(data['step'], data['value'], \n",
    "                        label=label, color=colors[i], linewidth=2, alpha=0.8)\n",
    "        \n",
    "        ax1.set_xlabel('Timesteps', fontsize=11)\n",
    "        ax1.set_ylabel('Episode Reward', fontsize=11)\n",
    "        ax1.set_title('Learning Curves', fontsize=12)\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Subplot 2: Improvement over baseline (absolute difference)\n",
    "        ax2 = axes[1]\n",
    "        \n",
    "        # Collect all improvement curves for averaging\n",
    "        all_improvements = []\n",
    "        all_common_steps = []\n",
    "        \n",
    "        for i, exp in enumerate(transfer_exps):\n",
    "            if exp['target_logs'] is None or not os.path.exists(exp['target_logs']):\n",
    "                continue\n",
    "            \n",
    "            transfer_data = load_tensorboard_data(exp['target_logs'], metric)\n",
    "            if len(transfer_data) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Interpolate both curves to common timesteps\n",
    "            common_steps = sorted(set(baseline_data['step']).intersection(set(transfer_data['step'])))\n",
    "            if len(common_steps) == 0:\n",
    "                # Use all steps and interpolate\n",
    "                all_steps = sorted(set(list(baseline_data['step']) + list(transfer_data['step'])))\n",
    "                baseline_interp = np.interp(all_steps, baseline_data['step'], baseline_data['value'])\n",
    "                transfer_interp = np.interp(all_steps, transfer_data['step'], transfer_data['value'])\n",
    "                common_steps = all_steps\n",
    "            else:\n",
    "                baseline_interp = np.interp(common_steps, baseline_data['step'], baseline_data['value'])\n",
    "                transfer_interp = np.interp(common_steps, transfer_data['step'], transfer_data['value'])\n",
    "            \n",
    "            # Calculate improvement (positive = better than baseline)\n",
    "            improvement = transfer_interp - baseline_interp\n",
    "            \n",
    "            # Plot individual curve (more transparent)\n",
    "            label = f\"{exp['source']} \u2192 {target_game}\"\n",
    "            ax2.plot(common_steps, improvement, \n",
    "                    label=label, color=colors[i], linewidth=1.5, alpha=0.4)\n",
    "            \n",
    "            # Store for averaging\n",
    "            all_improvements.append(improvement)\n",
    "            all_common_steps.append(common_steps)\n",
    "        \n",
    "        # Compute and plot average improvement\n",
    "        if len(all_improvements) > 1:\n",
    "            # Find common timesteps across all experiments\n",
    "            min_len = min(len(steps) for steps in all_common_steps)\n",
    "            # Truncate all to same length and compute mean\n",
    "            truncated_improvements = [imp[:min_len] for imp in all_improvements]\n",
    "            avg_improvement = np.mean(truncated_improvements, axis=0)\n",
    "            avg_steps = all_common_steps[0][:min_len]\n",
    "            \n",
    "            # Smooth the average\n",
    "            smoothed_avg = smooth_curve(avg_improvement, window_size=smooth_window)\n",
    "            \n",
    "            ax2.plot(avg_steps, smoothed_avg, \n",
    "                    label='Average (smoothed)', \n",
    "                    color='black', linewidth=3, linestyle='-', alpha=0.9, zorder=100)\n",
    "        \n",
    "        ax2.axhline(y=0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "        ax2.set_xlabel('Timesteps', fontsize=11)\n",
    "        ax2.set_ylabel('Reward Improvement vs Baseline', fontsize=11)\n",
    "        ax2.set_title('Absolute Improvement (Transfer - Baseline)', fontsize=12)\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Subplot 3: Relative improvement (percentage)\n",
    "        ax3 = axes[2]\n",
    "        \n",
    "        # Collect all relative improvement curves for averaging\n",
    "        all_rel_improvements = []\n",
    "        \n",
    "        for i, exp in enumerate(transfer_exps):\n",
    "            if exp['target_logs'] is None or not os.path.exists(exp['target_logs']):\n",
    "                continue\n",
    "            \n",
    "            transfer_data = load_tensorboard_data(exp['target_logs'], metric)\n",
    "            if len(transfer_data) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Interpolate both curves to common timesteps\n",
    "            common_steps = sorted(set(baseline_data['step']).intersection(set(transfer_data['step'])))\n",
    "            if len(common_steps) == 0:\n",
    "                all_steps = sorted(set(list(baseline_data['step']) + list(transfer_data['step'])))\n",
    "                baseline_interp = np.interp(all_steps, baseline_data['step'], baseline_data['value'])\n",
    "                transfer_interp = np.interp(all_steps, transfer_data['step'], transfer_data['value'])\n",
    "                common_steps = all_steps\n",
    "            else:\n",
    "                baseline_interp = np.interp(common_steps, baseline_data['step'], baseline_data['value'])\n",
    "                transfer_interp = np.interp(common_steps, transfer_data['step'], transfer_data['value'])\n",
    "            \n",
    "            # Calculate relative improvement (handle division by zero)\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                relative_improvement = np.where(\n",
    "                    np.abs(baseline_interp) > 1e-6,\n",
    "                    ((transfer_interp - baseline_interp) / np.abs(baseline_interp)) * 100,\n",
    "                    0\n",
    "                )\n",
    "            \n",
    "            # Plot individual curve (more transparent)\n",
    "            label = f\"{exp['source']} \u2192 {target_game}\"\n",
    "            ax3.plot(common_steps, relative_improvement, \n",
    "                    label=label, color=colors[i], linewidth=1.5, alpha=0.4)\n",
    "            \n",
    "            # Store for averaging\n",
    "            all_rel_improvements.append(relative_improvement)\n",
    "        \n",
    "        # Compute and plot average relative improvement\n",
    "        if len(all_rel_improvements) > 1:\n",
    "            # Find common length\n",
    "            min_len = min(len(imp) for imp in all_rel_improvements)\n",
    "            truncated_rel_improvements = [imp[:min_len] for imp in all_rel_improvements]\n",
    "            avg_rel_improvement = np.mean(truncated_rel_improvements, axis=0)\n",
    "            avg_steps = all_common_steps[0][:min_len]\n",
    "            \n",
    "            # Smooth the average\n",
    "            smoothed_avg_rel = smooth_curve(avg_rel_improvement, window_size=smooth_window)\n",
    "            \n",
    "            ax3.plot(avg_steps, smoothed_avg_rel, \n",
    "                    label='Average (smoothed)', \n",
    "                    color='black', linewidth=3, linestyle='-', alpha=0.9, zorder=100)\n",
    "        \n",
    "        ax3.axhline(y=0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "        ax3.set_xlabel('Timesteps', fontsize=11)\n",
    "        ax3.set_ylabel('Relative Improvement (%)', fontsize=11)\n",
    "        ax3.set_title('Relative Improvement ((Transfer - Baseline) / |Baseline| \u00d7 100)', fontsize=12)\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f'transfer_vs_baseline_{algo}_{target_game}.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "# Plot transfer vs baseline comparisons\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRANSFER VS BASELINE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Smoothing window size: 10 (adjustable via smooth_window parameter)\")\n",
    "plot_transfer_vs_baseline(experiments, baseline_experiments, smooth_window=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transfer Benefit Analysis\n",
    "\n",
    "Compute transfer benefit as the improvement over from-scratch baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_final_performance(log_dir, metric=\"eval/mean_reward\", last_n_steps=50000):\n",
    "    \"\"\"\n",
    "    Compute average performance over the last N steps.\n",
    "    \"\"\"\n",
    "    data = load_tensorboard_data(log_dir, metric)\n",
    "    if len(data) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Get data from last N steps\n",
    "    max_step = data['step'].max()\n",
    "    final_data = data[data['step'] >= (max_step - last_n_steps)]\n",
    "    \n",
    "    if len(final_data) == 0:\n",
    "        return data['value'].mean()\n",
    "    \n",
    "    return final_data['value'].mean()\n",
    "\n",
    "\n",
    "def compute_transfer_benefits(experiments):\n",
    "    \"\"\"\n",
    "    Compute transfer benefit for each experiment.\n",
    "    \n",
    "    Transfer benefit = (target performance - baseline) / |baseline| * 100\n",
    "    where baseline is the source game's from-scratch performance.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for exp in experiments:\n",
    "        # Get target performance (with transfer)\n",
    "        if exp['target_logs'] is None or not os.path.exists(exp['target_logs']):\n",
    "            continue\n",
    "        \n",
    "        target_perf = compute_final_performance(exp['target_logs'])\n",
    "        if target_perf is None:\n",
    "            continue\n",
    "        \n",
    "        # Get baseline performance (source game from scratch)\n",
    "        # This is the source_logs of the same experiment\n",
    "        baseline_perf = None\n",
    "        if exp['source_logs'] and os.path.exists(exp['source_logs']):\n",
    "            baseline_perf = compute_final_performance(exp['source_logs'])\n",
    "        \n",
    "        if baseline_perf is None or baseline_perf == 0:\n",
    "            continue\n",
    "        \n",
    "        # Compute transfer benefit\n",
    "        benefit = ((target_perf - baseline_perf) / abs(baseline_perf)) * 100\n",
    "        \n",
    "        results.append({\n",
    "            'algorithm': exp['algorithm'],\n",
    "            'source': exp['source'],\n",
    "            'target': exp['target'],\n",
    "            'pretrained': exp['pretrained'],\n",
    "            'baseline_performance': baseline_perf,\n",
    "            'target_performance': target_perf,\n",
    "            'transfer_benefit_pct': benefit\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Compute transfer benefits\n",
    "benefits_df = compute_transfer_benefits(experiments)\n",
    "\n",
    "if len(benefits_df) > 0:\n",
    "    print(\"\\nTransfer Benefit Analysis:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(benefits_df.to_string(index=False))\n",
    "    \n",
    "    # Save to CSV\n",
    "    benefits_df.to_csv(os.path.join(OUTPUT_DIR, 'transfer_benefits.csv'), index=False)\n",
    "    print(f\"\\nSaved to {os.path.join(OUTPUT_DIR, 'transfer_benefits.csv')}\")\n",
    "else:\n",
    "    print(\"No transfer benefit data available yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transfer Benefit Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(benefits_df) > 0:\n",
    "    # Plot transfer benefits by algorithm\n",
    "    algorithms = benefits_df['algorithm'].unique()\n",
    "    \n",
    "    for algo in algorithms:\n",
    "        algo_df = benefits_df[benefits_df['algorithm'] == algo]\n",
    "        \n",
    "        if len(algo_df) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Create transfer pair labels\n",
    "        algo_df = algo_df.copy()\n",
    "        algo_df['transfer_pair'] = algo_df['source'] + ' \u2192 ' + algo_df['target']\n",
    "        \n",
    "        # Sort by benefit\n",
    "        algo_df = algo_df.sort_values('transfer_benefit_pct')\n",
    "        \n",
    "        # Plot\n",
    "        fig, ax = plt.subplots(figsize=(12, max(6, len(algo_df) * 0.4)))\n",
    "        \n",
    "        colors = ['green' if x > 0 else 'red' for x in algo_df['transfer_benefit_pct']]\n",
    "        bars = ax.barh(algo_df['transfer_pair'], algo_df['transfer_benefit_pct'], color=colors, alpha=0.7)\n",
    "        \n",
    "        ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "        ax.set_xlabel('Transfer Benefit (%)', fontsize=12)\n",
    "        ax.set_ylabel('Transfer Pair', fontsize=12)\n",
    "        ax.set_title(f'{algo.upper()} Transfer Benefit Analysis', fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (idx, row) in enumerate(algo_df.iterrows()):\n",
    "            value = row['transfer_benefit_pct']\n",
    "            x_pos = value + (5 if value > 0 else -5)\n",
    "            ha = 'left' if value > 0 else 'right'\n",
    "            ax.text(x_pos, i, f'{value:.1f}%', ha=ha, va='center', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f'transfer_benefit_{algo}.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRANSFER BENEFIT SUMMARY BY ALGORITHM\")\n",
    "    print(\"=\"*80)\n",
    "    summary = benefits_df.groupby('algorithm')['transfer_benefit_pct'].agg(['mean', 'std', 'min', 'max', 'count'])\n",
    "    summary.columns = ['Mean (%)', 'Std (%)', 'Min (%)', 'Max (%)', 'N']\n",
    "    print(summary.to_string())\n",
    "else:\n",
    "    print(\"No transfer benefit data to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transfer Matrix Heatmap\n",
    "\n",
    "Create a heatmap showing transfer benefits between all game pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(benefits_df) > 0:\n",
    "    algorithms = benefits_df['algorithm'].unique()\n",
    "    \n",
    "    for algo in algorithms:\n",
    "        algo_df = benefits_df[benefits_df['algorithm'] == algo]\n",
    "        \n",
    "        if len(algo_df) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Create pivot table for heatmap\n",
    "        games = sorted(set(algo_df['source'].tolist() + algo_df['target'].tolist()))\n",
    "        matrix = pd.DataFrame(index=games, columns=games, dtype=float)\n",
    "        \n",
    "        for _, row in algo_df.iterrows():\n",
    "            matrix.loc[row['source'], row['target']] = row['transfer_benefit_pct']\n",
    "        \n",
    "        # Plot heatmap\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        \n",
    "        sns.heatmap(matrix, annot=True, fmt=\".1f\", cmap=\"RdYlGn\", center=0,\n",
    "                   cbar_kws={'label': 'Transfer Benefit (%)'}, ax=ax,\n",
    "                   linewidths=0.5, linecolor='gray')\n",
    "        \n",
    "        ax.set_xlabel('Target Game', fontsize=12)\n",
    "        ax.set_ylabel('Source Game', fontsize=12)\n",
    "        ax.set_title(f'{algo.upper()} Transfer Learning Matrix\\n(Source \u2192 Target)', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f'transfer_matrix_{algo}.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No data available for transfer matrix.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Pretrained vs From-Scratch Source\n",
    "\n",
    "If you have both pretrained and from-scratch experiments, compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(benefits_df) > 0 and 'pretrained' in benefits_df.columns:\n",
    "    # Check if we have both pretrained and non-pretrained experiments\n",
    "    has_pretrained = benefits_df['pretrained'].any()\n",
    "    has_scratch = (~benefits_df['pretrained']).any()\n",
    "    \n",
    "    if has_pretrained and has_scratch:\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        for algo in benefits_df['algorithm'].unique():\n",
    "            algo_df = benefits_df[benefits_df['algorithm'] == algo]\n",
    "            \n",
    "            pretrained_mean = algo_df[algo_df['pretrained']]['transfer_benefit_pct'].mean()\n",
    "            scratch_mean = algo_df[~algo_df['pretrained']]['transfer_benefit_pct'].mean()\n",
    "            \n",
    "            x = np.arange(2)\n",
    "            width = 0.25\n",
    "            offset = list(benefits_df['algorithm'].unique()).index(algo) * width\n",
    "            \n",
    "            ax.bar(x + offset, [scratch_mean, pretrained_mean], width, \n",
    "                  label=algo.upper(), alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Source Model Type', fontsize=12)\n",
    "        ax.set_ylabel('Average Transfer Benefit (%)', fontsize=12)\n",
    "        ax.set_title('Pretrained vs From-Scratch Source Models', fontsize=14, fontweight='bold')\n",
    "        ax.set_xticks([0.25, 1.25])\n",
    "        ax.set_xticklabels(['From Scratch', 'Pretrained (Zoo)'])\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        ax.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, 'pretrained_vs_scratch.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Need both pretrained and from-scratch experiments for comparison.\")\n",
    "else:\n",
    "    print(\"No pretrained comparison data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "All plots have been saved to the `analysis_plots/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal experiments analyzed: {len(experiments)}\")\n",
    "print(f\"Algorithms: {', '.join(sorted(set(exp['algorithm'] for exp in experiments)))}\")\n",
    "print(f\"\\nPlots saved to: {OUTPUT_DIR}/\")\n",
    "print(\"\\nGenerated files:\")\n",
    "for f in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Configuration for phased experiments\nPHASED_DIR = \"results_phased\"  # or \"/mnt/home/slee1/ceph/atari_phased_results\"\n\nprint(f\"Phased experiments directory: {PHASED_DIR}\")\nprint(f\"Looking for experiments in: {os.path.abspath(PHASED_DIR) if os.path.exists(PHASED_DIR) else 'NOT FOUND'}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Configuration for phased experiments\n",
    "PHASED_DIR = \"results_phased\"  # or \"/mnt/home/slee1/ceph/atari_phased_results\"\n",
    "\n",
    "print(f\"Phased experiments directory: {PHASED_DIR}\")\n",
    "print(f\"Looking for experiments in: {os.path.abspath(PHASED_DIR) if os.path.exists(PHASED_DIR) else 'NOT FOUND'}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def find_phased_experiments(phased_dir):\n",
    "    \"\"\"\n",
    "    Find phased transfer learning experiments.\n",
    "    \n",
    "    Returns:\n",
    "        source_experiments: Dict of source training experiments\n",
    "            key: (algorithm, source_game)\n",
    "            value: {path, checkpoints, logs, phases}\n",
    "        transfer_experiments: List of transfer experiments\n",
    "            [{algorithm, source, target, phase, checkpoint, path, logs}, ...]\n",
    "    \"\"\"\n",
    "    if not os.path.exists(phased_dir):\n",
    "        print(f\"Phased directory not found: {phased_dir}\")\n",
    "        return {}, []\n",
    "    \n",
    "    source_experiments = {}\n",
    "    transfer_experiments = []\n",
    "    \n",
    "    for exp_name in os.listdir(phased_dir):\n",
    "        exp_path = os.path.join(phased_dir, exp_name)\n",
    "        \n",
    "        if not os.path.isdir(exp_path):\n",
    "            continue\n",
    "        \n",
    "        # Skip script directories\n",
    "        if 'slurm' in exp_name:\n",
    "            continue\n",
    "        \n",
    "        # Parse source experiments: {algorithm}_{game}_source\n",
    "        if exp_name.endswith('_source'):\n",
    "            parts = exp_name.split('_')\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            \n",
    "            algorithm = parts[0]\n",
    "            source_game = parts[1]\n",
    "            \n",
    "            # Find checkpoints\n",
    "            checkpoint_dir = os.path.join(exp_path, 'checkpoints')\n",
    "            logs_dir = os.path.join(exp_path, 'logs')\n",
    "            \n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                continue\n",
    "            \n",
    "            # Find phase checkpoints\n",
    "            phase_checkpoints = {}\n",
    "            for ckpt_file in os.listdir(checkpoint_dir):\n",
    "                if ckpt_file.startswith('phase_') and ckpt_file.endswith('_model.zip'):\n",
    "                    # Extract phase number\n",
    "                    phase_str = ckpt_file.replace('phase_', '').replace('_model.zip', '')\n",
    "                    try:\n",
    "                        phase_num = int(phase_str)\n",
    "                        phase_checkpoints[phase_num] = os.path.join(checkpoint_dir, ckpt_file)\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "            \n",
    "            key = (algorithm, source_game)\n",
    "            source_experiments[key] = {\n",
    "                'algorithm': algorithm,\n",
    "                'source_game': source_game,\n",
    "                'path': exp_path,\n",
    "                'checkpoints': phase_checkpoints,\n",
    "                'logs': logs_dir if os.path.exists(logs_dir) else None,\n",
    "                'num_phases': len(phase_checkpoints)\n",
    "            }\n",
    "        \n",
    "        # Parse transfer experiments: {algorithm}_{source}_to_{target}_from_{checkpoint}_{jobid}\n",
    "        elif '_to_' in exp_name and '_from_' in exp_name:\n",
    "            parts = exp_name.split('_')\n",
    "            \n",
    "            try:\n",
    "                to_idx = parts.index('to')\n",
    "                from_idx = parts.index('from')\n",
    "                \n",
    "                algorithm = parts[0]\n",
    "                source_game = parts[to_idx - 1]\n",
    "                target_game = parts[to_idx + 1]\n",
    "                \n",
    "                # Extract phase from checkpoint name\n",
    "                # Checkpoint name format: phase_X_model\n",
    "                checkpoint_parts = parts[from_idx + 1:]\n",
    "                if len(checkpoint_parts) >= 2 and checkpoint_parts[0] == 'phase':\n",
    "                    try:\n",
    "                        phase_num = int(checkpoint_parts[1])\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                # Find logs\n",
    "                logs_dir = os.path.join(exp_path, 'logs')\n",
    "                \n",
    "                transfer_experiments.append({\n",
    "                    'algorithm': algorithm,\n",
    "                    'source': source_game,\n",
    "                    'target': target_game,\n",
    "                    'phase': phase_num,\n",
    "                    'checkpoint_name': '_'.join(checkpoint_parts[:3]),  # phase_X_model\n",
    "                    'path': exp_path,\n",
    "                    'logs': logs_dir if os.path.exists(logs_dir) else None,\n",
    "                    'name': exp_name\n",
    "                })\n",
    "            except (ValueError, IndexError):\n",
    "                continue\n",
    "    \n",
    "    return source_experiments, transfer_experiments\n",
    "\n",
    "\n",
    "# Load phased experiments\n",
    "phased_sources, phased_transfers = find_phased_experiments(PHASED_DIR)\n",
    "\n",
    "print(f\"\\nFound {len(phased_sources)} source training experiments\")\n",
    "print(f\"Found {len(phased_transfers)} transfer experiments\\n\")\n",
    "\n",
    "if phased_sources:\n",
    "    print(\"Source experiments:\")\n",
    "    for (algo, game), exp in sorted(phased_sources.items()):\n",
    "        print(f\"  {algo.upper()}/{game}: {exp['num_phases']} phases\")\n",
    "\n",
    "if phased_transfers:\n",
    "    print(f\"\\nTransfer experiments by phase:\")\n",
    "    transfer_df = pd.DataFrame(phased_transfers)\n",
    "    phase_summary = transfer_df.groupby(['algorithm', 'phase']).size().unstack(fill_value=0)\n",
    "    print(phase_summary)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phased Analysis: Transfer Performance by Phase\n",
    "\n",
    "For each phase checkpoint, compare baseline vs transfer performance."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_phased_transfer_comparison(phased_sources, phased_transfers,\n",
    "                                    metric=\"eval/mean_reward\", smooth_window=10):\n",
    "    \"\"\"\n",
    "    Plot transfer performance by phase compared to baseline.\n",
    "    \n",
    "    For each target game:\n",
    "    - Show baseline: target game trained from scratch (from phased_sources)\n",
    "    - Show transfer from each source at each phase\n",
    "    - Compare how performance changes with more source training\n",
    "    \n",
    "    Args:\n",
    "        phased_sources: Source training experiments (also used as baseline)\n",
    "        phased_transfers: Transfer experiments\n",
    "        metric: TensorBoard metric to plot\n",
    "        smooth_window: Smoothing window size\n",
    "    \"\"\"\n",
    "    # Group transfers by algorithm and target\n",
    "    transfers_by_target = {}\n",
    "    for transfer in phased_transfers:\n",
    "        key = (transfer['algorithm'], transfer['target'])\n",
    "        if key not in transfers_by_target:\n",
    "            transfers_by_target[key] = []\n",
    "        transfers_by_target[key].append(transfer)\n",
    "    \n",
    "    for (algo, target_game), transfers in sorted(transfers_by_target.items()):\n",
    "        # Use phased source training of TARGET game as baseline\n",
    "        baseline_key = (algo, target_game)\n",
    "        if baseline_key not in phased_sources:\n",
    "            print(f\"Skipping {algo}/{target_game}: No source training found for target game\")\n",
    "            print(f\"  (Looking for {algo}_{target_game}_source in phased results)\")\n",
    "            continue\n",
    "        \n",
    "        baseline_source = phased_sources[baseline_key]\n",
    "        if baseline_source['logs'] is None or not os.path.exists(baseline_source['logs']):\n",
    "            print(f\"Skipping {algo}/{target_game}: No logs found at {baseline_source.get('logs')}\")\n",
    "            continue\n",
    "        \n",
    "        baseline_data = load_tensorboard_data(baseline_source['logs'], metric)\n",
    "        \n",
    "        if len(baseline_data) == 0:\n",
    "            print(f\"Skipping {algo}/{target_game}: No baseline data in logs\")\n",
    "            continue\n",
    "        \n",
    "        # Show how much baseline training has completed\n",
    "        max_baseline_step = baseline_data['step'].max()\n",
    "        print(f\"\\n{algo.upper()}/{target_game}: Baseline training up to {max_baseline_step:,} steps\")\n",
    "        if baseline_source['num_phases'] > 0:\n",
    "            phases_complete = len([p for p in baseline_source['checkpoints'].keys()])\n",
    "            print(f\"  Phases completed: {phases_complete}/{baseline_source['num_phases']}\")\n",
    "        \n",
    "        # Group transfers by source and phase\n",
    "        transfers_by_source = {}\n",
    "        for transfer in transfers:\n",
    "            source = transfer['source']\n",
    "            # Skip self-transfer (e.g., Breakout\u2192Breakout)\n",
    "            if source == target_game:\n",
    "                continue\n",
    "            if source not in transfers_by_source:\n",
    "                transfers_by_source[source] = {}\n",
    "            phase = transfer['phase']\n",
    "            transfers_by_source[source][phase] = transfer\n",
    "        \n",
    "        if not transfers_by_source:\n",
    "            print(f\"  No cross-game transfers found (all were self-transfers)\")\n",
    "            continue\n",
    "        \n",
    "        # Create figure - one subplot per source game\n",
    "        num_sources = len(transfers_by_source)\n",
    "        fig, axes = plt.subplots(1, num_sources, figsize=(8 * num_sources, 6))\n",
    "        if num_sources == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        fig.suptitle(f'{algo.upper()}: Transfer to {target_game} by Phase',\n",
    "                    fontsize=14, fontweight='bold')\n",
    "        \n",
    "        for ax, (source_game, phase_transfers) in zip(axes, sorted(transfers_by_source.items())):\n",
    "            # Plot baseline (target game trained from scratch)\n",
    "            ax.plot(baseline_data['step'], baseline_data['value'],\n",
    "                   label=f'{target_game} from scratch (baseline)',\n",
    "                   color='gray', linewidth=2.5, linestyle='--', alpha=0.8, zorder=100)\n",
    "            \n",
    "            # Plot each phase transfer\n",
    "            phases = sorted(phase_transfers.keys())\n",
    "            colors = plt.cm.viridis(np.linspace(0, 1, len(phases)))\n",
    "            \n",
    "            for i, phase in enumerate(phases):\n",
    "                transfer = phase_transfers[phase]\n",
    "                if transfer['logs'] is None or not os.path.exists(transfer['logs']):\n",
    "                    continue\n",
    "                \n",
    "                data = load_tensorboard_data(transfer['logs'], metric)\n",
    "                if len(data) == 0:\n",
    "                    continue\n",
    "                \n",
    "                label = f\"Phase {phase}\"\n",
    "                ax.plot(data['step'], data['value'],\n",
    "                       label=label, color=colors[i], linewidth=2, alpha=0.7)\n",
    "            \n",
    "            ax.set_xlabel('Timesteps', fontsize=11)\n",
    "            ax.set_ylabel('Episode Reward', fontsize=11)\n",
    "            ax.set_title(f'Source: {source_game} \u2192 {target_game}', fontsize=12)\n",
    "            ax.legend(loc='best')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f'phased_transfer_{algo}_{target_game}.png'),\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if phased_transfers and phased_sources:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PHASED TRANSFER ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nComparing transfer learning against from-scratch training of the target game.\")\n",
    "    print(f\"Using phased experiments from: {PHASED_DIR}\")\n",
    "    print(\"\\nBaseline: Target game trained from scratch (phased source training)\")\n",
    "    print(\"Transfer: Target game trained from source checkpoints at each phase\")\n",
    "    plot_phased_transfer_comparison(phased_sources, phased_transfers)\n",
    "else:\n",
    "    print(\"Need both phased sources and phased transfers.\")\n",
    "    if not phased_sources:\n",
    "        print(\"  Missing: phased source experiments\")\n",
    "    if not phased_transfers:\n",
    "        print(\"  Missing: phased transfer experiments\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Performance by Phase\n",
    "\n",
    "Analyze how final transfer performance changes with more source training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def analyze_performance_by_phase(phased_sources, phased_transfers,\n",
    "                                metric=\"eval/mean_reward\", last_n_steps=50000):\n",
    "    \"\"\"\n",
    "    Analyze how transfer performance changes with phase.\n",
    "    \n",
    "    Uses target game's phased source training as baseline.\n",
    "    \n",
    "    Returns DataFrame with columns:\n",
    "    - algorithm, source, target, phase\n",
    "    - baseline_perf, transfer_perf, improvement, improvement_pct\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for transfer in phased_transfers:\n",
    "        # Skip self-transfers\n",
    "        if transfer['source'] == transfer['target']:\n",
    "            continue\n",
    "        \n",
    "        if transfer['logs'] is None or not os.path.exists(transfer['logs']):\n",
    "            continue\n",
    "        \n",
    "        # Get transfer performance\n",
    "        transfer_perf = compute_final_performance(transfer['logs'], metric, last_n_steps)\n",
    "        if transfer_perf is None:\n",
    "            continue\n",
    "        \n",
    "        # Get baseline performance from target game's source training\n",
    "        baseline_key = (transfer['algorithm'], transfer['target'])\n",
    "        if baseline_key not in phased_sources:\n",
    "            continue\n",
    "        \n",
    "        baseline_source = phased_sources[baseline_key]\n",
    "        if baseline_source['logs'] is None:\n",
    "            continue\n",
    "        \n",
    "        baseline_perf = compute_final_performance(baseline_source['logs'], metric, last_n_steps)\n",
    "        \n",
    "        if baseline_perf is None or abs(baseline_perf) < 1e-6:\n",
    "            continue\n",
    "        \n",
    "        # Compute improvement\n",
    "        improvement = transfer_perf - baseline_perf\n",
    "        improvement_pct = (improvement / abs(baseline_perf)) * 100\n",
    "        \n",
    "        results.append({\n",
    "            'algorithm': transfer['algorithm'],\n",
    "            'source': transfer['source'],\n",
    "            'target': transfer['target'],\n",
    "            'phase': transfer['phase'],\n",
    "            'baseline_perf': baseline_perf,\n",
    "            'transfer_perf': transfer_perf,\n",
    "            'improvement': improvement,\n",
    "            'improvement_pct': improvement_pct\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Analyze performance by phase\n",
    "if phased_transfers and phased_sources:\n",
    "    phased_results = analyze_performance_by_phase(phased_sources, phased_transfers)\n",
    "    \n",
    "    if len(phased_results) > 0:\n",
    "        print(\"\\nPerformance by Phase:\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"Baseline: Target game trained from scratch (phased source)\")\n",
    "        print(\"Transfer: Target game trained from source checkpoint\\n\")\n",
    "        print(phased_results.to_string(index=False))\n",
    "        \n",
    "        # Save to CSV\n",
    "        phased_results.to_csv(os.path.join(OUTPUT_DIR, 'phased_transfer_results.csv'), index=False)\n",
    "        print(f\"\\nSaved to {os.path.join(OUTPUT_DIR, 'phased_transfer_results.csv')}\")\n",
    "    else:\n",
    "        print(\"No phased results data available yet.\")\n",
    "        print(\"This could mean:\")\n",
    "        print(\"  - Transfer experiments haven't completed\")\n",
    "        print(\"  - Only self-transfers exist (e.g., Pong\u2192Pong)\")\n",
    "        print(\"  - TensorBoard logs are missing or incomplete\")\n",
    "else:\n",
    "    print(\"Need phased sources and transfers.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase Progression Plot\n",
    "\n",
    "Show how transfer benefit changes with more source training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if 'phased_results' in locals() and len(phased_results) > 0:\n",
    "    # Group by algorithm\n",
    "    for algo in phased_results['algorithm'].unique():\n",
    "        algo_df = phased_results[phased_results['algorithm'] == algo]\n",
    "        \n",
    "        # Group by source-target pair\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        fig.suptitle(f'{algo.upper()}: Transfer Performance vs Source Training Phase',\n",
    "                    fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Plot 1: Absolute improvement\n",
    "        ax1 = axes[0]\n",
    "        for (source, target), group in algo_df.groupby(['source', 'target']):\n",
    "            group = group.sort_values('phase')\n",
    "            label = f\"{source} \u2192 {target}\"\n",
    "            ax1.plot(group['phase'], group['improvement'],\n",
    "                    marker='o', label=label, linewidth=2, markersize=8)\n",
    "        \n",
    "        ax1.axhline(y=0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "        ax1.set_xlabel('Source Training Phase', fontsize=11)\n",
    "        ax1.set_ylabel('Improvement vs Baseline (Reward)', fontsize=11)\n",
    "        ax1.set_title('Absolute Improvement', fontsize=12)\n",
    "        ax1.legend(loc='best')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Relative improvement\n",
    "        ax2 = axes[1]\n",
    "        for (source, target), group in algo_df.groupby(['source', 'target']):\n",
    "            group = group.sort_values('phase')\n",
    "            label = f\"{source} \u2192 {target}\"\n",
    "            ax2.plot(group['phase'], group['improvement_pct'],\n",
    "                    marker='o', label=label, linewidth=2, markersize=8)\n",
    "        \n",
    "        ax2.axhline(y=0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "        ax2.set_xlabel('Source Training Phase', fontsize=11)\n",
    "        ax2.set_ylabel('Improvement vs Baseline (%)', fontsize=11)\n",
    "        ax2.set_title('Relative Improvement', fontsize=12)\n",
    "        ax2.legend(loc='best')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f'phased_progression_{algo}.png'),\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(f\"\\n{algo.upper()} - Summary by Phase:\")\n",
    "        print(\"=\"*80)\n",
    "        phase_summary = algo_df.groupby('phase').agg({\n",
    "            'improvement_pct': ['mean', 'std', 'min', 'max'],\n",
    "            'transfer_perf': 'mean'\n",
    "        })\n",
    "        phase_summary.columns = ['Mean Improvement (%)', 'Std (%)', 'Min (%)', 'Max (%)', 'Mean Transfer Perf']\n",
    "        print(phase_summary.to_string())\n",
    "else:\n",
    "    print(\"No phased results available for progression plot.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap: Transfer Benefit by Phase\n",
    "\n",
    "Visualize how different source games transfer to targets at each phase."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if 'phased_results' in locals() and len(phased_results) > 0:\n",
    "    for algo in phased_results['algorithm'].unique():\n",
    "        algo_df = phased_results[phased_results['algorithm'] == algo]\n",
    "        \n",
    "        # Get unique phases\n",
    "        phases = sorted(algo_df['phase'].unique())\n",
    "        \n",
    "        # Create subplots - one per phase\n",
    "        num_phases = len(phases)\n",
    "        cols = min(3, num_phases)\n",
    "        rows = (num_phases + cols - 1) // cols\n",
    "        \n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(7 * cols, 6 * rows))\n",
    "        if num_phases == 1:\n",
    "            axes = np.array([axes])\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        fig.suptitle(f'{algo.upper()}: Transfer Benefit Heatmap by Phase',\n",
    "                    fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for i, phase in enumerate(phases):\n",
    "            phase_df = algo_df[algo_df['phase'] == phase]\n",
    "            \n",
    "            # Create pivot table\n",
    "            sources = sorted(phase_df['source'].unique())\n",
    "            targets = sorted(phase_df['target'].unique())\n",
    "            \n",
    "            matrix = pd.DataFrame(index=sources, columns=targets, dtype=float)\n",
    "            for _, row in phase_df.iterrows():\n",
    "                matrix.loc[row['source'], row['target']] = row['improvement_pct']\n",
    "            \n",
    "            # Plot heatmap\n",
    "            ax = axes[i]\n",
    "            sns.heatmap(matrix, annot=True, fmt=\".1f\", cmap=\"RdYlGn\", center=0,\n",
    "                       cbar_kws={'label': 'Improvement (%)'},\n",
    "                       linewidths=0.5, linecolor='gray', ax=ax)\n",
    "            ax.set_xlabel('Target Game', fontsize=10)\n",
    "            ax.set_ylabel('Source Game', fontsize=10)\n",
    "            ax.set_title(f'Phase {phase}', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Hide extra subplots\n",
    "        for i in range(num_phases, len(axes)):\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f'phased_heatmap_{algo}.png'),\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No phased results for heatmap.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phased Analysis Summary\n",
    "\n",
    "All phased experiment plots have been saved."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASED ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'phased_results' in locals() and len(phased_results) > 0:\n",
    "    print(f\"\\nTotal phased experiments analyzed: {len(phased_results)}\")\n",
    "    print(f\"Algorithms: {', '.join(sorted(phased_results['algorithm'].unique()))}\")\n",
    "    print(f\"Phases: {sorted(phased_results['phase'].unique())}\")\n",
    "    print(f\"\\nPhased plots saved to: {OUTPUT_DIR}/\")\n",
    "    \n",
    "    phased_files = [f for f in os.listdir(OUTPUT_DIR) if 'phased' in f]\n",
    "    if phased_files:\n",
    "        print(\"\\nGenerated phased analysis files:\")\n",
    "        for f in sorted(phased_files):\n",
    "            print(f\"  - {f}\")\n",
    "else:\n",
    "    print(\"\\nNo phased experiments found or not yet complete.\")\n",
    "    print(f\"Make sure {PHASED_DIR} contains completed phased experiments.\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}