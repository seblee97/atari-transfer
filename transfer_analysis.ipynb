{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning Analysis for Atari Games\n",
    "\n",
    "This notebook analyzes transfer learning results including:\n",
    "1. Learning curves for each experiment\n",
    "2. Transfer benefit analysis (comparing transfer vs from-scratch performance)\n",
    "3. Visualizations split by algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Define the results directory and experiment parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "RESULTS_DIR = \"results\"\n",
    "OUTPUT_DIR = \"analysis_plots\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Games and algorithms from your config\n",
    "GAMES = [\"Pong\", \"Breakout\", \"SpaceInvaders\", \"Tennis\"]\n",
    "ALGORITHMS = [\"dqn\", \"ppo\", \"qrdqn\"]\n",
    "\n",
    "# Metrics to extract from TensorBoard\n",
    "METRICS = [\n",
    "    \"rollout/ep_rew_mean\",  # Episode reward\n",
    "    \"rollout/ep_len_mean\",  # Episode length\n",
    "    \"train/loss\",            # Training loss\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_tensorboard_data(log_dir, metric=\"rollout/ep_rew_mean\"):\n    \"\"\"\n    Load data from TensorBoard event files.\n    \n    Args:\n        log_dir: Path to TensorBoard log directory\n        metric: Metric name to extract\n    \n    Returns:\n        DataFrame with columns: step, value\n    \"\"\"\n    try:\n        ea = event_accumulator.EventAccumulator(log_dir)\n        ea.Reload()\n        \n        if metric not in ea.Tags()['scalars']:\n            print(f\"Warning: Metric '{metric}' not found in {log_dir}\")\n            return pd.DataFrame(columns=['step', 'value'])\n        \n        events = ea.Scalars(metric)\n        data = pd.DataFrame([\n            {'step': e.step, 'value': e.value}\n            for e in events\n        ])\n        return data\n    except Exception as e:\n        print(f\"Error loading {log_dir}: {e}\")\n        return pd.DataFrame(columns=['step', 'value'])\n\n\ndef parse_experiment_name(exp_name):\n    \"\"\"\n    Parse experiment name to extract metadata.\n    \n    Expected format: {algorithm}_{source}_to_{target}_{timestamp}\n    or: {algorithm}_{source}_to_{target}_pretrained_{timestamp}\n    \n    Returns:\n        dict with algorithm, source, target, pretrained, timestamp\n    \"\"\"\n    parts = exp_name.split('_')\n    \n    if len(parts) < 4:\n        return None\n    \n    algorithm = parts[0]\n    \n    # Find 'to' index\n    try:\n        to_idx = parts.index('to')\n    except ValueError:\n        return None\n    \n    source = parts[to_idx - 1]\n    \n    # Check if pretrained\n    if 'pretrained' in parts:\n        pretrained_idx = parts.index('pretrained')\n        target = parts[to_idx + 1]\n        pretrained = True\n        timestamp = '_'.join(parts[pretrained_idx + 1:])\n    else:\n        target = parts[to_idx + 1]\n        pretrained = False\n        timestamp = '_'.join(parts[to_idx + 2:])\n    \n    return {\n        'algorithm': algorithm,\n        'source': source,\n        'target': target,\n        'pretrained': pretrained,\n        'timestamp': timestamp\n    }\n\n\ndef find_experiments(results_dir, pretrained_only=True):\n    \"\"\"\n    Find all experiments in the results directory.\n    \n    Args:\n        results_dir: Path to results directory\n        pretrained_only: If True, only include pretrained experiments\n    \n    Returns:\n        List of dicts with experiment metadata and paths\n        - If multiple experiments exist for the same (algorithm, source, target) \n          combination, only the latest one (by timestamp) is kept\n    \"\"\"\n    experiments = []\n    \n    if not os.path.exists(results_dir):\n        print(f\"Results directory '{results_dir}' not found!\")\n        return experiments\n    \n    for exp_name in os.listdir(results_dir):\n        exp_path = os.path.join(results_dir, exp_name)\n        \n        if not os.path.isdir(exp_path):\n            continue\n        \n        # Skip non-experiment directories\n        if exp_name in ['slurm_scripts', 'slurm_scripts_pretrained', 'slurm_logs']:\n            continue\n        \n        metadata = parse_experiment_name(exp_name)\n        if metadata is None:\n            continue\n        \n        # Filter for pretrained only if requested\n        if pretrained_only and not metadata['pretrained']:\n            continue\n        \n        # Find log directories\n        source_logs = os.path.join(exp_path, 'source_logs')\n        target_logs = os.path.join(exp_path, 'target_logs')\n        \n        metadata['name'] = exp_name\n        metadata['path'] = exp_path\n        metadata['source_logs'] = source_logs if os.path.exists(source_logs) else None\n        metadata['target_logs'] = target_logs if os.path.exists(target_logs) else None\n        \n        experiments.append(metadata)\n    \n    # Keep only the latest experiment for each (algorithm, source, target) combination\n    # Group by (algorithm, source, target)\n    grouped = {}\n    for exp in experiments:\n        key = (exp['algorithm'], exp['source'], exp['target'])\n        if key not in grouped:\n            grouped[key] = []\n        grouped[key].append(exp)\n    \n    # For each group, keep only the one with the latest timestamp\n    latest_experiments = []\n    for key, exps in grouped.items():\n        # Sort by timestamp (lexicographic sort works for format YYYYMMDD_HHMMSS)\n        latest_exp = max(exps, key=lambda e: e['timestamp'])\n        latest_experiments.append(latest_exp)\n    \n    print(f\"\\nFiltering: pretrained_only={pretrained_only}\")\n    print(f\"Found {len(experiments)} total experiments\")\n    print(f\"Kept {len(latest_experiments)} latest unique experiments\")\n    \n    # Show which experiments were kept/discarded\n    if len(experiments) > len(latest_experiments):\n        print(\"\\nDuplicate experiments found (keeping only latest):\")\n        kept_names = set(exp['name'] for exp in latest_experiments)\n        for exp in experiments:\n            if exp['name'] not in kept_names:\n                key = (exp['algorithm'], exp['source'], exp['target'])\n                print(f\"  Discarded: {exp['name']}\")\n    \n    return latest_experiments"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all experiments\n",
    "experiments = find_experiments(RESULTS_DIR)\n",
    "\n",
    "print(f\"Found {len(experiments)} experiments\")\n",
    "print(\"\\nExperiment summary:\")\n",
    "exp_df = pd.DataFrame(experiments)\n",
    "if len(exp_df) > 0:\n",
    "    print(exp_df[['algorithm', 'source', 'target', 'pretrained']].to_string())\n",
    "else:\n",
    "    print(\"No experiments found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Learning Curves\n",
    "\n",
    "Plot learning curves for each experiment, organized by algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves_by_algorithm(experiments, metric=\"rollout/ep_rew_mean\"):\n",
    "    \"\"\"\n",
    "    Plot learning curves grouped by algorithm.\n",
    "    \"\"\"\n",
    "    algorithms = sorted(set(exp['algorithm'] for exp in experiments))\n",
    "    \n",
    "    for algo in algorithms:\n",
    "        algo_exps = [exp for exp in experiments if exp['algorithm'] == algo]\n",
    "        \n",
    "        if not algo_exps:\n",
    "            continue\n",
    "        \n",
    "        # Create subplots for source and target\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        fig.suptitle(f'{algo.upper()} Learning Curves - {metric}', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Plot source training\n",
    "        ax_source = axes[0]\n",
    "        for exp in algo_exps:\n",
    "            if exp['source_logs'] is None or not os.path.exists(exp['source_logs']):\n",
    "                continue\n",
    "            \n",
    "            data = load_tensorboard_data(exp['source_logs'], metric)\n",
    "            if len(data) > 0:\n",
    "                label = f\"{exp['source']}\"\n",
    "                ax_source.plot(data['step'], data['value'], label=label, alpha=0.7)\n",
    "        \n",
    "        ax_source.set_xlabel('Timesteps')\n",
    "        ax_source.set_ylabel('Reward')\n",
    "        ax_source.set_title('Source Game Training (from scratch)')\n",
    "        ax_source.legend()\n",
    "        ax_source.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot target training\n",
    "        ax_target = axes[1]\n",
    "        for exp in algo_exps:\n",
    "            if exp['target_logs'] is None or not os.path.exists(exp['target_logs']):\n",
    "                continue\n",
    "            \n",
    "            data = load_tensorboard_data(exp['target_logs'], metric)\n",
    "            if len(data) > 0:\n",
    "                pretrained_tag = \" (pretrained)\" if exp['pretrained'] else \"\"\n",
    "                label = f\"{exp['source']} → {exp['target']}{pretrained_tag}\"\n",
    "                ax_target.plot(data['step'], data['value'], label=label, alpha=0.7)\n",
    "        \n",
    "        ax_target.set_xlabel('Timesteps')\n",
    "        ax_target.set_ylabel('Reward')\n",
    "        ax_target.set_title('Target Game Training (with transfer)')\n",
    "        ax_target.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        ax_target.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f'learning_curves_{algo}.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "# Plot learning curves\n",
    "plot_learning_curves_by_algorithm(experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Individual Transfer Comparisons\n",
    "\n",
    "For each transfer pair, compare source (baseline) vs target (transfer) performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_transfer_comparison(experiments, metric=\"rollout/ep_rew_mean\"):\n",
    "    \"\"\"\n",
    "    Plot source vs target learning curves for each transfer experiment.\n",
    "    \"\"\"\n",
    "    # Group by algorithm and transfer pair\n",
    "    transfer_pairs = {}\n",
    "    for exp in experiments:\n",
    "        key = (exp['algorithm'], exp['source'], exp['target'])\n",
    "        if key not in transfer_pairs:\n",
    "            transfer_pairs[key] = []\n",
    "        transfer_pairs[key].append(exp)\n",
    "    \n",
    "    for (algo, source, target), exps in sorted(transfer_pairs.items()):\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "        \n",
    "        # Plot source baseline (from-scratch performance on target game)\n",
    "        # This would be an experiment where source == target\n",
    "        baseline_exp = next(\n",
    "            (e for e in experiments \n",
    "             if e['algorithm'] == algo and e['source'] == target and e['target'] == target),\n",
    "            None\n",
    "        )\n",
    "        \n",
    "        if baseline_exp and baseline_exp['source_logs']:\n",
    "            baseline_data = load_tensorboard_data(baseline_exp['source_logs'], metric)\n",
    "            if len(baseline_data) > 0:\n",
    "                ax.plot(baseline_data['step'], baseline_data['value'], \n",
    "                       label=f'{target} (from scratch)', \n",
    "                       linestyle='--', linewidth=2, alpha=0.7)\n",
    "        \n",
    "        # Plot transfer learning curves\n",
    "        for exp in exps:\n",
    "            if exp['target_logs'] is None or not os.path.exists(exp['target_logs']):\n",
    "                continue\n",
    "            \n",
    "            data = load_tensorboard_data(exp['target_logs'], metric)\n",
    "            if len(data) > 0:\n",
    "                pretrained_tag = \" (pretrained)\" if exp['pretrained'] else \"\"\n",
    "                label = f'{source} → {target}{pretrained_tag}'\n",
    "                ax.plot(data['step'], data['value'], label=label, linewidth=2, alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Timesteps', fontsize=12)\n",
    "        ax.set_ylabel('Episode Reward', fontsize=12)\n",
    "        ax.set_title(f'{algo.upper()}: {source} → {target} Transfer Learning', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f'transfer_{algo}_{source}_to_{target}.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "# Plot transfer comparisons\n",
    "plot_transfer_comparison(experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transfer Benefit Analysis\n",
    "\n",
    "Compute transfer benefit as the improvement over from-scratch baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_final_performance(log_dir, metric=\"rollout/ep_rew_mean\", last_n_steps=50000):\n",
    "    \"\"\"\n",
    "    Compute average performance over the last N steps.\n",
    "    \"\"\"\n",
    "    data = load_tensorboard_data(log_dir, metric)\n",
    "    if len(data) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Get data from last N steps\n",
    "    max_step = data['step'].max()\n",
    "    final_data = data[data['step'] >= (max_step - last_n_steps)]\n",
    "    \n",
    "    if len(final_data) == 0:\n",
    "        return data['value'].mean()\n",
    "    \n",
    "    return final_data['value'].mean()\n",
    "\n",
    "\n",
    "def compute_transfer_benefits(experiments):\n",
    "    \"\"\"\n",
    "    Compute transfer benefit for each experiment.\n",
    "    \n",
    "    Transfer benefit = (target performance - baseline) / |baseline| * 100\n",
    "    where baseline is the source game's from-scratch performance.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for exp in experiments:\n",
    "        # Get target performance (with transfer)\n",
    "        if exp['target_logs'] is None or not os.path.exists(exp['target_logs']):\n",
    "            continue\n",
    "        \n",
    "        target_perf = compute_final_performance(exp['target_logs'])\n",
    "        if target_perf is None:\n",
    "            continue\n",
    "        \n",
    "        # Get baseline performance (source game from scratch)\n",
    "        # This is the source_logs of the same experiment\n",
    "        baseline_perf = None\n",
    "        if exp['source_logs'] and os.path.exists(exp['source_logs']):\n",
    "            baseline_perf = compute_final_performance(exp['source_logs'])\n",
    "        \n",
    "        if baseline_perf is None or baseline_perf == 0:\n",
    "            continue\n",
    "        \n",
    "        # Compute transfer benefit\n",
    "        benefit = ((target_perf - baseline_perf) / abs(baseline_perf)) * 100\n",
    "        \n",
    "        results.append({\n",
    "            'algorithm': exp['algorithm'],\n",
    "            'source': exp['source'],\n",
    "            'target': exp['target'],\n",
    "            'pretrained': exp['pretrained'],\n",
    "            'baseline_performance': baseline_perf,\n",
    "            'target_performance': target_perf,\n",
    "            'transfer_benefit_pct': benefit\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Compute transfer benefits\n",
    "benefits_df = compute_transfer_benefits(experiments)\n",
    "\n",
    "if len(benefits_df) > 0:\n",
    "    print(\"\\nTransfer Benefit Analysis:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(benefits_df.to_string(index=False))\n",
    "    \n",
    "    # Save to CSV\n",
    "    benefits_df.to_csv(os.path.join(OUTPUT_DIR, 'transfer_benefits.csv'), index=False)\n",
    "    print(f\"\\nSaved to {os.path.join(OUTPUT_DIR, 'transfer_benefits.csv')}\")\n",
    "else:\n",
    "    print(\"No transfer benefit data available yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transfer Benefit Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(benefits_df) > 0:\n",
    "    # Plot transfer benefits by algorithm\n",
    "    algorithms = benefits_df['algorithm'].unique()\n",
    "    \n",
    "    for algo in algorithms:\n",
    "        algo_df = benefits_df[benefits_df['algorithm'] == algo]\n",
    "        \n",
    "        if len(algo_df) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Create transfer pair labels\n",
    "        algo_df = algo_df.copy()\n",
    "        algo_df['transfer_pair'] = algo_df['source'] + ' → ' + algo_df['target']\n",
    "        \n",
    "        # Sort by benefit\n",
    "        algo_df = algo_df.sort_values('transfer_benefit_pct')\n",
    "        \n",
    "        # Plot\n",
    "        fig, ax = plt.subplots(figsize=(12, max(6, len(algo_df) * 0.4)))\n",
    "        \n",
    "        colors = ['green' if x > 0 else 'red' for x in algo_df['transfer_benefit_pct']]\n",
    "        bars = ax.barh(algo_df['transfer_pair'], algo_df['transfer_benefit_pct'], color=colors, alpha=0.7)\n",
    "        \n",
    "        ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "        ax.set_xlabel('Transfer Benefit (%)', fontsize=12)\n",
    "        ax.set_ylabel('Transfer Pair', fontsize=12)\n",
    "        ax.set_title(f'{algo.upper()} Transfer Benefit Analysis', fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (idx, row) in enumerate(algo_df.iterrows()):\n",
    "            value = row['transfer_benefit_pct']\n",
    "            x_pos = value + (5 if value > 0 else -5)\n",
    "            ha = 'left' if value > 0 else 'right'\n",
    "            ax.text(x_pos, i, f'{value:.1f}%', ha=ha, va='center', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f'transfer_benefit_{algo}.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRANSFER BENEFIT SUMMARY BY ALGORITHM\")\n",
    "    print(\"=\"*80)\n",
    "    summary = benefits_df.groupby('algorithm')['transfer_benefit_pct'].agg(['mean', 'std', 'min', 'max', 'count'])\n",
    "    summary.columns = ['Mean (%)', 'Std (%)', 'Min (%)', 'Max (%)', 'N']\n",
    "    print(summary.to_string())\n",
    "else:\n",
    "    print(\"No transfer benefit data to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transfer Matrix Heatmap\n",
    "\n",
    "Create a heatmap showing transfer benefits between all game pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(benefits_df) > 0:\n",
    "    algorithms = benefits_df['algorithm'].unique()\n",
    "    \n",
    "    for algo in algorithms:\n",
    "        algo_df = benefits_df[benefits_df['algorithm'] == algo]\n",
    "        \n",
    "        if len(algo_df) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Create pivot table for heatmap\n",
    "        games = sorted(set(algo_df['source'].tolist() + algo_df['target'].tolist()))\n",
    "        matrix = pd.DataFrame(index=games, columns=games, dtype=float)\n",
    "        \n",
    "        for _, row in algo_df.iterrows():\n",
    "            matrix.loc[row['source'], row['target']] = row['transfer_benefit_pct']\n",
    "        \n",
    "        # Plot heatmap\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        \n",
    "        sns.heatmap(matrix, annot=True, fmt=\".1f\", cmap=\"RdYlGn\", center=0,\n",
    "                   cbar_kws={'label': 'Transfer Benefit (%)'}, ax=ax,\n",
    "                   linewidths=0.5, linecolor='gray')\n",
    "        \n",
    "        ax.set_xlabel('Target Game', fontsize=12)\n",
    "        ax.set_ylabel('Source Game', fontsize=12)\n",
    "        ax.set_title(f'{algo.upper()} Transfer Learning Matrix\\n(Source → Target)', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f'transfer_matrix_{algo}.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No data available for transfer matrix.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Pretrained vs From-Scratch Source\n",
    "\n",
    "If you have both pretrained and from-scratch experiments, compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(benefits_df) > 0 and 'pretrained' in benefits_df.columns:\n",
    "    # Check if we have both pretrained and non-pretrained experiments\n",
    "    has_pretrained = benefits_df['pretrained'].any()\n",
    "    has_scratch = (~benefits_df['pretrained']).any()\n",
    "    \n",
    "    if has_pretrained and has_scratch:\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        for algo in benefits_df['algorithm'].unique():\n",
    "            algo_df = benefits_df[benefits_df['algorithm'] == algo]\n",
    "            \n",
    "            pretrained_mean = algo_df[algo_df['pretrained']]['transfer_benefit_pct'].mean()\n",
    "            scratch_mean = algo_df[~algo_df['pretrained']]['transfer_benefit_pct'].mean()\n",
    "            \n",
    "            x = np.arange(2)\n",
    "            width = 0.25\n",
    "            offset = list(benefits_df['algorithm'].unique()).index(algo) * width\n",
    "            \n",
    "            ax.bar(x + offset, [scratch_mean, pretrained_mean], width, \n",
    "                  label=algo.upper(), alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Source Model Type', fontsize=12)\n",
    "        ax.set_ylabel('Average Transfer Benefit (%)', fontsize=12)\n",
    "        ax.set_title('Pretrained vs From-Scratch Source Models', fontsize=14, fontweight='bold')\n",
    "        ax.set_xticks([0.25, 1.25])\n",
    "        ax.set_xticklabels(['From Scratch', 'Pretrained (Zoo)'])\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        ax.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, 'pretrained_vs_scratch.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Need both pretrained and from-scratch experiments for comparison.\")\n",
    "else:\n",
    "    print(\"No pretrained comparison data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "All plots have been saved to the `analysis_plots/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal experiments analyzed: {len(experiments)}\")\n",
    "print(f\"Algorithms: {', '.join(sorted(set(exp['algorithm'] for exp in experiments)))}\")\n",
    "print(f\"\\nPlots saved to: {OUTPUT_DIR}/\")\n",
    "print(\"\\nGenerated files:\")\n",
    "for f in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    print(f\"  - {f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}